//go:build !noasm && amd64
// Code generated by GoAT. DO NOT EDIT.
// versions:
// 	clang   19.1.7 (++20250114103320+cd708029e0b2-1~exp1~20250114103432.75)
// 	objdump 2.38
// flags: -mavx -O3
// source: src/floats_avx.c

TEXT ·_mm256_mul_const_add_to(SB), $0-40
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ c+16(FP), DX
	MOVQ dst+24(FP), CX
	MOVQ n+32(FP), R8
	BYTE $0x55                                 // pushq	%rbp
	WORD $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	LONG $0xf8e48348                           // andq	$-8, %rsp
	LONG $0x07488d4d                           // leaq	7(%r8), %r9
	WORD $0x854d; BYTE $0xc0                   // testq	%r8, %r8
	LONG $0xc8490f4d                           // cmovnsq	%r8, %r9
	WORD $0x894c; BYTE $0xc8                   // movq	%r9, %rax
	LONG $0x03f8c148                           // sarq	$3, %rax
	LONG $0xf8e18349                           // andq	$-8, %r9
	WORD $0x294d; BYTE $0xc8                   // subq	%r9, %r8
	WORD $0xc085                               // testl	%eax, %eax
	JLE  LBB0_6
	WORD $0xf883; BYTE $0x01                   // cmpl	$1, %eax
	JE   LBB0_4
	WORD $0x8941; BYTE $0xc1                   // movl	%eax, %r9d
	LONG $0xfee18141; WORD $0xffff; BYTE $0x7f // andl	$2147483646, %r9d               # imm = 0x7FFFFFFE

LBB0_3:
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0258fcc5             // vaddps	(%rdx), %ymm0, %ymm0
	LONG $0x0111fcc5             // vmovups	%ymm0, (%rcx)
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x4759fcc5; BYTE $0x20 // vmulps	32(%rdi), %ymm0, %ymm0
	LONG $0x4258fcc5; BYTE $0x20 // vaddps	32(%rdx), %ymm0, %ymm0
	LONG $0x4111fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rcx)
	LONG $0x40c78348             // addq	$64, %rdi
	LONG $0x40c28348             // addq	$64, %rdx
	LONG $0x40c18348             // addq	$64, %rcx
	LONG $0xfec18341             // addl	$-2, %r9d
	JNE  LBB0_3

LBB0_4:
	WORD $0x01a8                 // testb	$1, %al
	JE   LBB0_6
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0258fcc5             // vaddps	(%rdx), %ymm0, %ymm0
	LONG $0x0111fcc5             // vmovups	%ymm0, (%rcx)
	LONG $0x20c78348             // addq	$32, %rdi
	LONG $0x20c28348             // addq	$32, %rdx
	LONG $0x20c18348             // addq	$32, %rcx

LBB0_6:
	WORD $0x854d; BYTE $0xc0     // testq	%r8, %r8
	JLE  LBB0_14
	WORD $0x8944; BYTE $0xc0     // movl	%r8d, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x0258fac5             // vaddss	(%rdx), %xmm0, %xmm0
	LONG $0x0111fac5             // vmovss	%xmm0, (%rcx)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB0_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x04 // vaddss	4(%rdx), %xmm0, %xmm0
	LONG $0x4111fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rcx)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB0_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x08 // vaddss	8(%rdx), %xmm0, %xmm0
	LONG $0x4111fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rcx)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB0_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x0c // vaddss	12(%rdx), %xmm0, %xmm0
	LONG $0x4111fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rcx)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB0_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x10 // vaddss	16(%rdx), %xmm0, %xmm0
	LONG $0x4111fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rcx)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB0_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x14 // vaddss	20(%rdx), %xmm0, %xmm0
	LONG $0x4111fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rcx)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB0_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x18 // vaddss	24(%rdx), %xmm0, %xmm0
	LONG $0x4111fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rcx)

LBB0_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_mul_const_add(SB), $0-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ c+16(FP), DX
	MOVQ n+24(FP), CX
	BYTE $0x55                                 // pushq	%rbp
	WORD $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	LONG $0xf8e48348                           // andq	$-8, %rsp
	LONG $0x07418d4c                           // leaq	7(%rcx), %r8
	WORD $0x8548; BYTE $0xc9                   // testq	%rcx, %rcx
	LONG $0xc1490f4c                           // cmovnsq	%rcx, %r8
	WORD $0x894c; BYTE $0xc0                   // movq	%r8, %rax
	LONG $0x03f8c148                           // sarq	$3, %rax
	LONG $0xf8e08349                           // andq	$-8, %r8
	WORD $0x294c; BYTE $0xc1                   // subq	%r8, %rcx
	WORD $0xc085                               // testl	%eax, %eax
	JLE  LBB1_6
	WORD $0xf883; BYTE $0x01                   // cmpl	$1, %eax
	JE   LBB1_4
	WORD $0x8941; BYTE $0xc0                   // movl	%eax, %r8d
	LONG $0xfee08141; WORD $0xffff; BYTE $0x7f // andl	$2147483646, %r8d               # imm = 0x7FFFFFFE

LBB1_3:
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0258fcc5             // vaddps	(%rdx), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x4759fcc5; BYTE $0x20 // vmulps	32(%rdi), %ymm0, %ymm0
	LONG $0x4258fcc5; BYTE $0x20 // vaddps	32(%rdx), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdx)
	LONG $0x40c78348             // addq	$64, %rdi
	LONG $0x40c28348             // addq	$64, %rdx
	LONG $0xfec08341             // addl	$-2, %r8d
	JNE  LBB1_3

LBB1_4:
	WORD $0x01a8                 // testb	$1, %al
	JE   LBB1_6
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0258fcc5             // vaddps	(%rdx), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x20c78348             // addq	$32, %rdi
	LONG $0x20c28348             // addq	$32, %rdx

LBB1_6:
	WORD $0x8548; BYTE $0xc9     // testq	%rcx, %rcx
	JLE  LBB1_14
	WORD $0xc889                 // movl	%ecx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x0258fac5             // vaddss	(%rdx), %xmm0, %xmm0
	LONG $0x0211fac5             // vmovss	%xmm0, (%rdx)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB1_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x04 // vaddss	4(%rdx), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdx)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB1_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x08 // vaddss	8(%rdx), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdx)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB1_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x0c // vaddss	12(%rdx), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdx)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB1_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x10 // vaddss	16(%rdx), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdx)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB1_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x14 // vaddss	20(%rdx), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdx)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB1_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4258fac5; BYTE $0x18 // vaddss	24(%rdx), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdx)

LBB1_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_mul_const_to(SB), $0-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ c+16(FP), DX
	MOVQ n+24(FP), CX
	BYTE $0x55                                 // pushq	%rbp
	WORD $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	LONG $0xf8e48348                           // andq	$-8, %rsp
	LONG $0x07418d4c                           // leaq	7(%rcx), %r8
	WORD $0x8548; BYTE $0xc9                   // testq	%rcx, %rcx
	LONG $0xc1490f4c                           // cmovnsq	%rcx, %r8
	WORD $0x894c; BYTE $0xc0                   // movq	%r8, %rax
	LONG $0x03f8c148                           // sarq	$3, %rax
	LONG $0xf8e08349                           // andq	$-8, %r8
	WORD $0x294c; BYTE $0xc1                   // subq	%r8, %rcx
	WORD $0xc085                               // testl	%eax, %eax
	JLE  LBB2_6
	WORD $0xf883; BYTE $0x01                   // cmpl	$1, %eax
	JE   LBB2_4
	WORD $0x8941; BYTE $0xc0                   // movl	%eax, %r8d
	LONG $0xfee08141; WORD $0xffff; BYTE $0x7f // andl	$2147483646, %r8d               # imm = 0x7FFFFFFE

LBB2_3:
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x4759fcc5; BYTE $0x20 // vmulps	32(%rdi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdx)
	LONG $0x40c78348             // addq	$64, %rdi
	LONG $0x40c28348             // addq	$64, %rdx
	LONG $0xfec08341             // addl	$-2, %r8d
	JNE  LBB2_3

LBB2_4:
	WORD $0x01a8                 // testb	$1, %al
	JE   LBB2_6
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x20c78348             // addq	$32, %rdi
	LONG $0x20c28348             // addq	$32, %rdx

LBB2_6:
	WORD $0x8548; BYTE $0xc9     // testq	%rcx, %rcx
	JLE  LBB2_14
	WORD $0xc889                 // movl	%ecx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x0211fac5             // vmovss	%xmm0, (%rdx)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB2_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdx)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB2_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdx)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB2_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdx)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB2_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdx)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB2_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdx)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB2_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdx)

LBB2_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_mul_const(SB), $0-24
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ n+16(FP), DX
	BYTE $0x55                     // pushq	%rbp
	WORD $0x8948; BYTE $0xe5       // movq	%rsp, %rbp
	LONG $0xf8e48348               // andq	$-8, %rsp
	LONG $0x074a8d48               // leaq	7(%rdx), %rcx
	WORD $0x8548; BYTE $0xd2       // testq	%rdx, %rdx
	LONG $0xca490f48               // cmovnsq	%rdx, %rcx
	WORD $0x8948; BYTE $0xc8       // movq	%rcx, %rax
	LONG $0x03f8c148               // sarq	$3, %rax
	LONG $0xf8e18348               // andq	$-8, %rcx
	WORD $0x2948; BYTE $0xca       // subq	%rcx, %rdx
	WORD $0xc085                   // testl	%eax, %eax
	JLE  LBB3_6
	WORD $0xf883; BYTE $0x01       // cmpl	$1, %eax
	JE   LBB3_4
	WORD $0xc189                   // movl	%eax, %ecx
	LONG $0xfffee181; WORD $0x7fff // andl	$2147483646, %ecx               # imm = 0x7FFFFFFE

LBB3_3:
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0711fcc5             // vmovups	%ymm0, (%rdi)
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x4759fcc5; BYTE $0x20 // vmulps	32(%rdi), %ymm0, %ymm0
	LONG $0x4711fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdi)
	LONG $0x40c78348             // addq	$64, %rdi
	WORD $0xc183; BYTE $0xfe     // addl	$-2, %ecx
	JNE  LBB3_3

LBB3_4:
	WORD $0x01a8                 // testb	$1, %al
	JE   LBB3_6
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0759fcc5             // vmulps	(%rdi), %ymm0, %ymm0
	LONG $0x0711fcc5             // vmovups	%ymm0, (%rdi)
	LONG $0x20c78348             // addq	$32, %rdi

LBB3_6:
	WORD $0x8548; BYTE $0xd2     // testq	%rdx, %rdx
	JLE  LBB3_14
	WORD $0xd089                 // movl	%edx, %eax
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x0759fac5             // vmulss	(%rdi), %xmm0, %xmm0
	LONG $0x0711fac5             // vmovss	%xmm0, (%rdi)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB3_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4759fac5; BYTE $0x04 // vmulss	4(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdi)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB3_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4759fac5; BYTE $0x08 // vmulss	8(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdi)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB3_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4759fac5; BYTE $0x0c // vmulss	12(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdi)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB3_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4759fac5; BYTE $0x10 // vmulss	16(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdi)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB3_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4759fac5; BYTE $0x14 // vmulss	20(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdi)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB3_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4759fac5; BYTE $0x18 // vmulss	24(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdi)

LBB3_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_add_const(SB), $0-24
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ n+16(FP), DX
	BYTE $0x55                     // pushq	%rbp
	WORD $0x8948; BYTE $0xe5       // movq	%rsp, %rbp
	LONG $0xf8e48348               // andq	$-8, %rsp
	LONG $0x074a8d48               // leaq	7(%rdx), %rcx
	WORD $0x8548; BYTE $0xd2       // testq	%rdx, %rdx
	LONG $0xca490f48               // cmovnsq	%rdx, %rcx
	WORD $0x8948; BYTE $0xc8       // movq	%rcx, %rax
	LONG $0x03f8c148               // sarq	$3, %rax
	LONG $0xf8e18348               // andq	$-8, %rcx
	WORD $0x2948; BYTE $0xca       // subq	%rcx, %rdx
	WORD $0xc085                   // testl	%eax, %eax
	JLE  LBB4_6
	WORD $0xf883; BYTE $0x01       // cmpl	$1, %eax
	JE   LBB4_4
	WORD $0xc189                   // movl	%eax, %ecx
	LONG $0xfffee181; WORD $0x7fff // andl	$2147483646, %ecx               # imm = 0x7FFFFFFE

LBB4_3:
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0758fcc5             // vaddps	(%rdi), %ymm0, %ymm0
	LONG $0x0711fcc5             // vmovups	%ymm0, (%rdi)
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x4758fcc5; BYTE $0x20 // vaddps	32(%rdi), %ymm0, %ymm0
	LONG $0x4711fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdi)
	LONG $0x40c78348             // addq	$64, %rdi
	WORD $0xc183; BYTE $0xfe     // addl	$-2, %ecx
	JNE  LBB4_3

LBB4_4:
	WORD $0x01a8                 // testb	$1, %al
	JE   LBB4_6
	LONG $0x187de2c4; BYTE $0x06 // vbroadcastss	(%rsi), %ymm0
	LONG $0x0758fcc5             // vaddps	(%rdi), %ymm0, %ymm0
	LONG $0x0711fcc5             // vmovups	%ymm0, (%rdi)
	LONG $0x20c78348             // addq	$32, %rdi

LBB4_6:
	WORD $0x8548; BYTE $0xd2     // testq	%rdx, %rdx
	JLE  LBB4_14
	WORD $0xd089                 // movl	%edx, %eax
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x0758fac5             // vaddss	(%rdi), %xmm0, %xmm0
	LONG $0x0711fac5             // vmovss	%xmm0, (%rdi)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB4_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4758fac5; BYTE $0x04 // vaddss	4(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdi)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB4_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4758fac5; BYTE $0x08 // vaddss	8(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdi)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB4_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4758fac5; BYTE $0x0c // vaddss	12(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdi)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB4_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4758fac5; BYTE $0x10 // vaddss	16(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdi)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB4_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4758fac5; BYTE $0x14 // vaddss	20(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdi)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB4_14
	LONG $0x0610fac5             // vmovss	(%rsi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x4758fac5; BYTE $0x18 // vaddss	24(%rdi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdi)

LBB4_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_sub_to(SB), $0-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ c+16(FP), DX
	MOVQ n+24(FP), CX
	BYTE $0x55                                 // pushq	%rbp
	WORD $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	LONG $0xf8e48348                           // andq	$-8, %rsp
	LONG $0x07418d48                           // leaq	7(%rcx), %rax
	WORD $0x8548; BYTE $0xc9                   // testq	%rcx, %rcx
	LONG $0xc1490f48                           // cmovnsq	%rcx, %rax
	WORD $0x8949; BYTE $0xc0                   // movq	%rax, %r8
	LONG $0x03f8c149                           // sarq	$3, %r8
	LONG $0xf8e08348                           // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc1                   // subq	%rax, %rcx
	WORD $0x8545; BYTE $0xc0                   // testl	%r8d, %r8d
	JLE  LBB5_6
	WORD $0x8944; BYTE $0xc0                   // movl	%r8d, %eax
	WORD $0xe083; BYTE $0x03                   // andl	$3, %eax
	LONG $0x04f88341                           // cmpl	$4, %r8d
	JB   LBB5_4
	LONG $0xfce08141; WORD $0xffff; BYTE $0x7f // andl	$2147483644, %r8d               # imm = 0x7FFFFFFC

LBB5_3:
	LONG $0x0710fcc5             // vmovups	(%rdi), %ymm0
	LONG $0x065cfcc5             // vsubps	(%rsi), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x4710fcc5; BYTE $0x20 // vmovups	32(%rdi), %ymm0
	LONG $0x465cfcc5; BYTE $0x20 // vsubps	32(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdx)
	LONG $0x4710fcc5; BYTE $0x40 // vmovups	64(%rdi), %ymm0
	LONG $0x465cfcc5; BYTE $0x40 // vsubps	64(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x40 // vmovups	%ymm0, 64(%rdx)
	LONG $0x4710fcc5; BYTE $0x60 // vmovups	96(%rdi), %ymm0
	LONG $0x465cfcc5; BYTE $0x60 // vsubps	96(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x60 // vmovups	%ymm0, 96(%rdx)
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	LONG $0x80ea8348             // subq	$-128, %rdx
	LONG $0xfcc08341             // addl	$-4, %r8d
	JNE  LBB5_3

LBB5_4:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB5_6

LBB5_5:
	LONG $0x0710fcc5 // vmovups	(%rdi), %ymm0
	LONG $0x065cfcc5 // vsubps	(%rsi), %ymm0, %ymm0
	LONG $0x0211fcc5 // vmovups	%ymm0, (%rdx)
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	LONG $0x20c28348 // addq	$32, %rdx
	WORD $0xc8ff     // decl	%eax
	JNE  LBB5_5

LBB5_6:
	WORD $0x8548; BYTE $0xc9     // testq	%rcx, %rcx
	JLE  LBB5_14
	WORD $0xc889                 // movl	%ecx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x065cfac5             // vsubss	(%rsi), %xmm0, %xmm0
	LONG $0x0211fac5             // vmovss	%xmm0, (%rdx)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB5_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x04 // vsubss	4(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdx)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB5_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x08 // vsubss	8(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdx)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB5_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x0c // vsubss	12(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdx)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB5_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x10 // vsubss	16(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdx)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB5_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x14 // vsubss	20(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdx)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB5_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x18 // vsubss	24(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdx)

LBB5_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_sub(SB), $0-24
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ n+16(FP), DX
	BYTE $0x55                     // pushq	%rbp
	WORD $0x8948; BYTE $0xe5       // movq	%rsp, %rbp
	LONG $0xf8e48348               // andq	$-8, %rsp
	LONG $0x07428d48               // leaq	7(%rdx), %rax
	WORD $0x8548; BYTE $0xd2       // testq	%rdx, %rdx
	LONG $0xc2490f48               // cmovnsq	%rdx, %rax
	WORD $0x8948; BYTE $0xc1       // movq	%rax, %rcx
	LONG $0x03f9c148               // sarq	$3, %rcx
	LONG $0xf8e08348               // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc2       // subq	%rax, %rdx
	WORD $0xc985                   // testl	%ecx, %ecx
	JLE  LBB6_6
	WORD $0xc889                   // movl	%ecx, %eax
	WORD $0xe083; BYTE $0x03       // andl	$3, %eax
	WORD $0xf983; BYTE $0x04       // cmpl	$4, %ecx
	JB   LBB6_4
	LONG $0xfffce181; WORD $0x7fff // andl	$2147483644, %ecx               # imm = 0x7FFFFFFC

LBB6_3:
	LONG $0x0710fcc5             // vmovups	(%rdi), %ymm0
	LONG $0x4f10fcc5; BYTE $0x20 // vmovups	32(%rdi), %ymm1
	LONG $0x5710fcc5; BYTE $0x40 // vmovups	64(%rdi), %ymm2
	LONG $0x065cfcc5             // vsubps	(%rsi), %ymm0, %ymm0
	LONG $0x5f10fcc5; BYTE $0x60 // vmovups	96(%rdi), %ymm3
	LONG $0x0711fcc5             // vmovups	%ymm0, (%rdi)
	LONG $0x465cf4c5; BYTE $0x20 // vsubps	32(%rsi), %ymm1, %ymm0
	LONG $0x4711fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdi)
	LONG $0x465cecc5; BYTE $0x40 // vsubps	64(%rsi), %ymm2, %ymm0
	LONG $0x4711fcc5; BYTE $0x40 // vmovups	%ymm0, 64(%rdi)
	LONG $0x465ce4c5; BYTE $0x60 // vsubps	96(%rsi), %ymm3, %ymm0
	LONG $0x4711fcc5; BYTE $0x60 // vmovups	%ymm0, 96(%rdi)
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	WORD $0xc183; BYTE $0xfc     // addl	$-4, %ecx
	JNE  LBB6_3

LBB6_4:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB6_6

LBB6_5:
	LONG $0x0710fcc5 // vmovups	(%rdi), %ymm0
	LONG $0x065cfcc5 // vsubps	(%rsi), %ymm0, %ymm0
	LONG $0x0711fcc5 // vmovups	%ymm0, (%rdi)
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	WORD $0xc8ff     // decl	%eax
	JNE  LBB6_5

LBB6_6:
	WORD $0x8548; BYTE $0xd2     // testq	%rdx, %rdx
	JLE  LBB6_14
	WORD $0xd089                 // movl	%edx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x065cfac5             // vsubss	(%rsi), %xmm0, %xmm0
	LONG $0x0711fac5             // vmovss	%xmm0, (%rdi)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB6_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x04 // vsubss	4(%rsi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdi)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB6_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x08 // vsubss	8(%rsi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdi)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB6_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x0c // vsubss	12(%rsi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdi)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB6_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x10 // vsubss	16(%rsi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdi)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB6_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x14 // vsubss	20(%rsi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdi)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB6_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465cfac5; BYTE $0x18 // vsubss	24(%rsi), %xmm0, %xmm0
	LONG $0x4711fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdi)

LBB6_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_mul_to(SB), $0-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ c+16(FP), DX
	MOVQ n+24(FP), CX
	BYTE $0x55                                 // pushq	%rbp
	WORD $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	LONG $0xf8e48348                           // andq	$-8, %rsp
	LONG $0x07418d48                           // leaq	7(%rcx), %rax
	WORD $0x8548; BYTE $0xc9                   // testq	%rcx, %rcx
	LONG $0xc1490f48                           // cmovnsq	%rcx, %rax
	WORD $0x8949; BYTE $0xc0                   // movq	%rax, %r8
	LONG $0x03f8c149                           // sarq	$3, %r8
	LONG $0xf8e08348                           // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc1                   // subq	%rax, %rcx
	WORD $0x8545; BYTE $0xc0                   // testl	%r8d, %r8d
	JLE  LBB7_6
	WORD $0x8944; BYTE $0xc0                   // movl	%r8d, %eax
	WORD $0xe083; BYTE $0x03                   // andl	$3, %eax
	LONG $0x04f88341                           // cmpl	$4, %r8d
	JB   LBB7_4
	LONG $0xfce08141; WORD $0xffff; BYTE $0x7f // andl	$2147483644, %r8d               # imm = 0x7FFFFFFC

LBB7_3:
	LONG $0x0710fcc5             // vmovups	(%rdi), %ymm0
	LONG $0x0659fcc5             // vmulps	(%rsi), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x4710fcc5; BYTE $0x20 // vmovups	32(%rdi), %ymm0
	LONG $0x4659fcc5; BYTE $0x20 // vmulps	32(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdx)
	LONG $0x4710fcc5; BYTE $0x40 // vmovups	64(%rdi), %ymm0
	LONG $0x4659fcc5; BYTE $0x40 // vmulps	64(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x40 // vmovups	%ymm0, 64(%rdx)
	LONG $0x4710fcc5; BYTE $0x60 // vmovups	96(%rdi), %ymm0
	LONG $0x4659fcc5; BYTE $0x60 // vmulps	96(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x60 // vmovups	%ymm0, 96(%rdx)
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	LONG $0x80ea8348             // subq	$-128, %rdx
	LONG $0xfcc08341             // addl	$-4, %r8d
	JNE  LBB7_3

LBB7_4:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB7_6

LBB7_5:
	LONG $0x0710fcc5 // vmovups	(%rdi), %ymm0
	LONG $0x0659fcc5 // vmulps	(%rsi), %ymm0, %ymm0
	LONG $0x0211fcc5 // vmovups	%ymm0, (%rdx)
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	LONG $0x20c28348 // addq	$32, %rdx
	WORD $0xc8ff     // decl	%eax
	JNE  LBB7_5

LBB7_6:
	WORD $0x8548; BYTE $0xc9     // testq	%rcx, %rcx
	JLE  LBB7_14
	WORD $0xc889                 // movl	%ecx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x0659fac5             // vmulss	(%rsi), %xmm0, %xmm0
	LONG $0x0211fac5             // vmovss	%xmm0, (%rdx)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB7_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x4659fac5; BYTE $0x04 // vmulss	4(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdx)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB7_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x4659fac5; BYTE $0x08 // vmulss	8(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdx)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB7_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x4659fac5; BYTE $0x0c // vmulss	12(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdx)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB7_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x4659fac5; BYTE $0x10 // vmulss	16(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdx)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB7_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x4659fac5; BYTE $0x14 // vmulss	20(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdx)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB7_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x4659fac5; BYTE $0x18 // vmulss	24(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdx)

LBB7_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_div_to(SB), $0-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ c+16(FP), DX
	MOVQ n+24(FP), CX
	BYTE $0x55                                 // pushq	%rbp
	WORD $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	LONG $0xf8e48348                           // andq	$-8, %rsp
	LONG $0x07418d48                           // leaq	7(%rcx), %rax
	WORD $0x8548; BYTE $0xc9                   // testq	%rcx, %rcx
	LONG $0xc1490f48                           // cmovnsq	%rcx, %rax
	WORD $0x8949; BYTE $0xc0                   // movq	%rax, %r8
	LONG $0x03f8c149                           // sarq	$3, %r8
	LONG $0xf8e08348                           // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc1                   // subq	%rax, %rcx
	WORD $0x8545; BYTE $0xc0                   // testl	%r8d, %r8d
	JLE  LBB8_6
	WORD $0x8944; BYTE $0xc0                   // movl	%r8d, %eax
	WORD $0xe083; BYTE $0x03                   // andl	$3, %eax
	LONG $0x04f88341                           // cmpl	$4, %r8d
	JB   LBB8_4
	LONG $0xfce08141; WORD $0xffff; BYTE $0x7f // andl	$2147483644, %r8d               # imm = 0x7FFFFFFC

LBB8_3:
	LONG $0x0710fcc5             // vmovups	(%rdi), %ymm0
	LONG $0x065efcc5             // vdivps	(%rsi), %ymm0, %ymm0
	LONG $0x0211fcc5             // vmovups	%ymm0, (%rdx)
	LONG $0x4710fcc5; BYTE $0x20 // vmovups	32(%rdi), %ymm0
	LONG $0x465efcc5; BYTE $0x20 // vdivps	32(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rdx)
	LONG $0x4710fcc5; BYTE $0x40 // vmovups	64(%rdi), %ymm0
	LONG $0x465efcc5; BYTE $0x40 // vdivps	64(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x40 // vmovups	%ymm0, 64(%rdx)
	LONG $0x4710fcc5; BYTE $0x60 // vmovups	96(%rdi), %ymm0
	LONG $0x465efcc5; BYTE $0x60 // vdivps	96(%rsi), %ymm0, %ymm0
	LONG $0x4211fcc5; BYTE $0x60 // vmovups	%ymm0, 96(%rdx)
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	LONG $0x80ea8348             // subq	$-128, %rdx
	LONG $0xfcc08341             // addl	$-4, %r8d
	JNE  LBB8_3

LBB8_4:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB8_6

LBB8_5:
	LONG $0x0710fcc5 // vmovups	(%rdi), %ymm0
	LONG $0x065efcc5 // vdivps	(%rsi), %ymm0, %ymm0
	LONG $0x0211fcc5 // vmovups	%ymm0, (%rdx)
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	LONG $0x20c28348 // addq	$32, %rdx
	WORD $0xc8ff     // decl	%eax
	JNE  LBB8_5

LBB8_6:
	WORD $0x8548; BYTE $0xc9     // testq	%rcx, %rcx
	JLE  LBB8_14
	WORD $0xc889                 // movl	%ecx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0x065efac5             // vdivss	(%rsi), %xmm0, %xmm0
	LONG $0x0211fac5             // vmovss	%xmm0, (%rdx)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB8_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x465efac5; BYTE $0x04 // vdivss	4(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rdx)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB8_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0x465efac5; BYTE $0x08 // vdivss	8(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rdx)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB8_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465efac5; BYTE $0x0c // vdivss	12(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rdx)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB8_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465efac5; BYTE $0x10 // vdivss	16(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rdx)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB8_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465efac5; BYTE $0x14 // vdivss	20(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rdx)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB8_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0x465efac5; BYTE $0x18 // vdivss	24(%rsi), %xmm0, %xmm0
	LONG $0x4211fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rdx)

LBB8_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_sqrt_to(SB), $0-24
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ n+16(FP), DX
	BYTE $0x55                     // pushq	%rbp
	WORD $0x8948; BYTE $0xe5       // movq	%rsp, %rbp
	LONG $0xf8e48348               // andq	$-8, %rsp
	LONG $0x07428d48               // leaq	7(%rdx), %rax
	WORD $0x8548; BYTE $0xd2       // testq	%rdx, %rdx
	LONG $0xc2490f48               // cmovnsq	%rdx, %rax
	WORD $0x8948; BYTE $0xc1       // movq	%rax, %rcx
	LONG $0x03f9c148               // sarq	$3, %rcx
	LONG $0xf8e08348               // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc2       // subq	%rax, %rdx
	WORD $0xc985                   // testl	%ecx, %ecx
	JLE  LBB9_6
	WORD $0xc889                   // movl	%ecx, %eax
	WORD $0xe083; BYTE $0x03       // andl	$3, %eax
	WORD $0xf983; BYTE $0x04       // cmpl	$4, %ecx
	JB   LBB9_4
	LONG $0xfffce181; WORD $0x7fff // andl	$2147483644, %ecx               # imm = 0x7FFFFFFC

LBB9_3:
	LONG $0x0751fcc5             // vsqrtps	(%rdi), %ymm0
	LONG $0x0611fcc5             // vmovups	%ymm0, (%rsi)
	LONG $0x4751fcc5; BYTE $0x20 // vsqrtps	32(%rdi), %ymm0
	LONG $0x4611fcc5; BYTE $0x20 // vmovups	%ymm0, 32(%rsi)
	LONG $0x4751fcc5; BYTE $0x40 // vsqrtps	64(%rdi), %ymm0
	LONG $0x4611fcc5; BYTE $0x40 // vmovups	%ymm0, 64(%rsi)
	LONG $0x4751fcc5; BYTE $0x60 // vsqrtps	96(%rdi), %ymm0
	LONG $0x4611fcc5; BYTE $0x60 // vmovups	%ymm0, 96(%rsi)
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	WORD $0xc183; BYTE $0xfc     // addl	$-4, %ecx
	JNE  LBB9_3

LBB9_4:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB9_6

LBB9_5:
	LONG $0x0751fcc5 // vsqrtps	(%rdi), %ymm0
	LONG $0x0611fcc5 // vmovups	%ymm0, (%rsi)
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	WORD $0xc8ff     // decl	%eax
	JNE  LBB9_5

LBB9_6:
	WORD $0x8548; BYTE $0xd2     // testq	%rdx, %rdx
	JLE  LBB9_14
	WORD $0xd089                 // movl	%edx, %eax
	LONG $0x0710fac5             // vmovss	(%rdi), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x0611fac5             // vmovss	%xmm0, (%rsi)
	LONG $0x01f88348             // cmpq	$1, %rax
	JE   LBB9_14
	LONG $0x4710fac5; BYTE $0x04 // vmovss	4(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x4611fac5; BYTE $0x04 // vmovss	%xmm0, 4(%rsi)
	WORD $0xf883; BYTE $0x02     // cmpl	$2, %eax
	JE   LBB9_14
	LONG $0x4710fac5; BYTE $0x08 // vmovss	8(%rdi), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x4611fac5; BYTE $0x08 // vmovss	%xmm0, 8(%rsi)
	WORD $0xf883; BYTE $0x03     // cmpl	$3, %eax
	JE   LBB9_14
	LONG $0x4710fac5; BYTE $0x0c // vmovss	12(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x4611fac5; BYTE $0x0c // vmovss	%xmm0, 12(%rsi)
	WORD $0xf883; BYTE $0x04     // cmpl	$4, %eax
	JE   LBB9_14
	LONG $0x4710fac5; BYTE $0x10 // vmovss	16(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x4611fac5; BYTE $0x10 // vmovss	%xmm0, 16(%rsi)
	WORD $0xf883; BYTE $0x05     // cmpl	$5, %eax
	JE   LBB9_14
	LONG $0x4710fac5; BYTE $0x14 // vmovss	20(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x4611fac5; BYTE $0x14 // vmovss	%xmm0, 20(%rsi)
	WORD $0xf883; BYTE $0x06     // cmpl	$6, %eax
	JE   LBB9_14
	LONG $0x4710fac5; BYTE $0x18 // vmovss	24(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	LONG $0xc051fac5             // vsqrtss	%xmm0, %xmm0, %xmm0
	LONG $0x4611fac5; BYTE $0x18 // vmovss	%xmm0, 24(%rsi)

LBB9_14:
	WORD $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	RET

TEXT ·_mm256_dot(SB), $8-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ n+16(FP), DX
	BYTE $0x55               // pushq	%rbp
	WORD $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG $0xf8e48348         // andq	$-8, %rsp
	LONG $0x07428d48         // leaq	7(%rdx), %rax
	WORD $0x8548; BYTE $0xd2 // testq	%rdx, %rdx
	LONG $0xc2490f48         // cmovnsq	%rdx, %rax
	WORD $0x8949; BYTE $0xc0 // movq	%rax, %r8
	LONG $0x03f8c149         // sarq	$3, %r8
	LONG $0xf8e08348         // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc2 // subq	%rax, %rdx
	WORD $0x8545; BYTE $0xc0 // testl	%r8d, %r8d
	JLE  LBB10_1
	LONG $0x0710fcc5         // vmovups	(%rdi), %ymm0
	LONG $0x0659fcc5         // vmulps	(%rsi), %ymm0, %ymm0
	LONG $0x20c78348         // addq	$32, %rdi
	LONG $0x20c68348         // addq	$32, %rsi
	LONG $0x01f88341         // cmpl	$1, %r8d
	JE   LBB10_8
	LONG $0xff488d41         // leal	-1(%r8), %ecx
	LONG $0xfec08341         // addl	$-2, %r8d
	WORD $0xc889             // movl	%ecx, %eax
	WORD $0xe083; BYTE $0x03 // andl	$3, %eax
	LONG $0x03f88341         // cmpl	$3, %r8d
	JB   LBB10_6
	WORD $0xe183; BYTE $0xfc // andl	$-4, %ecx

LBB10_5:
	LONG $0x0f10fcc5             // vmovups	(%rdi), %ymm1
	LONG $0x5710fcc5; BYTE $0x20 // vmovups	32(%rdi), %ymm2
	LONG $0x5f10fcc5; BYTE $0x40 // vmovups	64(%rdi), %ymm3
	LONG $0x6710fcc5; BYTE $0x60 // vmovups	96(%rdi), %ymm4
	LONG $0x0e59f4c5             // vmulps	(%rsi), %ymm1, %ymm1
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x4e59ecc5; BYTE $0x20 // vmulps	32(%rsi), %ymm2, %ymm1
	LONG $0x5659e4c5; BYTE $0x40 // vmulps	64(%rsi), %ymm3, %ymm2
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0xc258fcc5             // vaddps	%ymm2, %ymm0, %ymm0
	LONG $0x4e59dcc5; BYTE $0x60 // vmulps	96(%rsi), %ymm4, %ymm1
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	WORD $0xc183; BYTE $0xfc     // addl	$-4, %ecx
	JNE  LBB10_5

LBB10_6:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB10_8

LBB10_7:
	LONG $0x0f10fcc5 // vmovups	(%rdi), %ymm1
	LONG $0x0e59f4c5 // vmulps	(%rsi), %ymm1, %ymm1
	LONG $0xc158fcc5 // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	WORD $0xc8ff     // decl	%eax
	JNE  LBB10_7
	JMP  LBB10_8

LBB10_1:
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0

LBB10_8:
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5               // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01   // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5               // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5               // vaddss	%xmm1, %xmm0, %xmm0
	WORD $0x8548; BYTE $0xd2       // testq	%rdx, %rdx
	JLE  LBB10_16
	WORD $0xd089                   // movl	%edx, %eax
	LONG $0x0f10fac5               // vmovss	(%rdi), %xmm1                   # xmm1 = mem[0],zero,zero,zero
	LONG $0x0e59f2c5               // vmulss	(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	LONG $0x01f88348               // cmpq	$1, %rax
	JE   LBB10_16
	LONG $0x4f10fac5; BYTE $0x04   // vmovss	4(%rdi), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0x04   // vmulss	4(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x02       // cmpl	$2, %eax
	JE   LBB10_16
	LONG $0x4f10fac5; BYTE $0x08   // vmovss	8(%rdi), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0x08   // vmulss	8(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x03       // cmpl	$3, %eax
	JE   LBB10_16
	LONG $0x4f10fac5; BYTE $0x0c   // vmovss	12(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0x0c   // vmulss	12(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x04       // cmpl	$4, %eax
	JE   LBB10_16
	LONG $0x4f10fac5; BYTE $0x10   // vmovss	16(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0x10   // vmulss	16(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x05       // cmpl	$5, %eax
	JE   LBB10_16
	LONG $0x4f10fac5; BYTE $0x14   // vmovss	20(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0x14   // vmulss	20(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x06       // cmpl	$6, %eax
	JE   LBB10_16
	LONG $0x4f10fac5; BYTE $0x18   // vmovss	24(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0x18   // vmulss	24(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0

LBB10_16:
	WORD  $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE  $0x5d               // popq	%rbp
	WORD  $0xf8c5; BYTE $0x77 // vzeroupper
	MOVSS X0, result+24(FP)
	RET

TEXT ·_mm256_euclidean(SB), $8-32
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ n+16(FP), DX
	BYTE $0x55               // pushq	%rbp
	WORD $0x8948; BYTE $0xe5 // movq	%rsp, %rbp
	LONG $0xf8e48348         // andq	$-8, %rsp
	LONG $0x07428d48         // leaq	7(%rdx), %rax
	WORD $0x8548; BYTE $0xd2 // testq	%rdx, %rdx
	LONG $0xc2490f48         // cmovnsq	%rdx, %rax
	WORD $0x8949; BYTE $0xc0 // movq	%rax, %r8
	LONG $0x03f8c149         // sarq	$3, %r8
	LONG $0xf8e08348         // andq	$-8, %rax
	WORD $0x2948; BYTE $0xc2 // subq	%rax, %rdx
	WORD $0x8545; BYTE $0xc0 // testl	%r8d, %r8d
	JLE  LBB11_1
	LONG $0x0710fcc5         // vmovups	(%rdi), %ymm0
	LONG $0x065cfcc5         // vsubps	(%rsi), %ymm0, %ymm0
	LONG $0xc059fcc5         // vmulps	%ymm0, %ymm0, %ymm0
	LONG $0x20c78348         // addq	$32, %rdi
	LONG $0x20c68348         // addq	$32, %rsi
	LONG $0x01f88341         // cmpl	$1, %r8d
	JE   LBB11_8
	LONG $0xff488d41         // leal	-1(%r8), %ecx
	LONG $0xfec08341         // addl	$-2, %r8d
	WORD $0xc889             // movl	%ecx, %eax
	WORD $0xe083; BYTE $0x03 // andl	$3, %eax
	LONG $0x03f88341         // cmpl	$3, %r8d
	JB   LBB11_6
	WORD $0xe183; BYTE $0xfc // andl	$-4, %ecx

LBB11_5:
	LONG $0x0f10fcc5             // vmovups	(%rdi), %ymm1
	LONG $0x5710fcc5; BYTE $0x20 // vmovups	32(%rdi), %ymm2
	LONG $0x5f10fcc5; BYTE $0x40 // vmovups	64(%rdi), %ymm3
	LONG $0x6710fcc5; BYTE $0x60 // vmovups	96(%rdi), %ymm4
	LONG $0x0e5cf4c5             // vsubps	(%rsi), %ymm1, %ymm1
	LONG $0xc959f4c5             // vmulps	%ymm1, %ymm1, %ymm1
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x4e5cecc5; BYTE $0x20 // vsubps	32(%rsi), %ymm2, %ymm1
	LONG $0xc959f4c5             // vmulps	%ymm1, %ymm1, %ymm1
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x4e5ce4c5; BYTE $0x40 // vsubps	64(%rsi), %ymm3, %ymm1
	LONG $0xc959f4c5             // vmulps	%ymm1, %ymm1, %ymm1
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x4e5cdcc5; BYTE $0x60 // vsubps	96(%rsi), %ymm4, %ymm1
	LONG $0xc959f4c5             // vmulps	%ymm1, %ymm1, %ymm1
	LONG $0xc158fcc5             // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x80ef8348             // subq	$-128, %rdi
	LONG $0x80ee8348             // subq	$-128, %rsi
	WORD $0xc183; BYTE $0xfc     // addl	$-4, %ecx
	JNE  LBB11_5

LBB11_6:
	WORD $0xc085 // testl	%eax, %eax
	JE   LBB11_8

LBB11_7:
	LONG $0x0f10fcc5 // vmovups	(%rdi), %ymm1
	LONG $0x0e5cf4c5 // vsubps	(%rsi), %ymm1, %ymm1
	LONG $0xc959f4c5 // vmulps	%ymm1, %ymm1, %ymm1
	LONG $0xc158fcc5 // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x20c78348 // addq	$32, %rdi
	LONG $0x20c68348 // addq	$32, %rsi
	WORD $0xc8ff     // decl	%eax
	JNE  LBB11_7
	JMP  LBB11_8

LBB11_1:
	LONG $0xc057f8c5 // vxorps	%xmm0, %xmm0, %xmm0

LBB11_8:
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5               // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01   // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5               // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5               // vaddss	%xmm1, %xmm0, %xmm0
	WORD $0x8548; BYTE $0xd2       // testq	%rdx, %rdx
	JLE  LBB11_16
	LONG $0x0f10fac5               // vmovss	(%rdi), %xmm1                   # xmm1 = mem[0],zero,zero,zero
	LONG $0x0e5cf2c5               // vsubss	(%rsi), %xmm1, %xmm1
	WORD $0xd089                   // movl	%edx, %eax
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	LONG $0x01f88348               // cmpq	$1, %rax
	JE   LBB11_16
	LONG $0x4f10fac5; BYTE $0x04   // vmovss	4(%rdi), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e5cf2c5; BYTE $0x04   // vsubss	4(%rsi), %xmm1, %xmm1
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x02       // cmpl	$2, %eax
	JE   LBB11_16
	LONG $0x4f10fac5; BYTE $0x08   // vmovss	8(%rdi), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e5cf2c5; BYTE $0x08   // vsubss	8(%rsi), %xmm1, %xmm1
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x03       // cmpl	$3, %eax
	JE   LBB11_16
	LONG $0x4f10fac5; BYTE $0x0c   // vmovss	12(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e5cf2c5; BYTE $0x0c   // vsubss	12(%rsi), %xmm1, %xmm1
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x04       // cmpl	$4, %eax
	JE   LBB11_16
	LONG $0x4f10fac5; BYTE $0x10   // vmovss	16(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e5cf2c5; BYTE $0x10   // vsubss	16(%rsi), %xmm1, %xmm1
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x05       // cmpl	$5, %eax
	JE   LBB11_16
	LONG $0x4f10fac5; BYTE $0x14   // vmovss	20(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e5cf2c5; BYTE $0x14   // vsubss	20(%rsi), %xmm1, %xmm1
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x06       // cmpl	$6, %eax
	JE   LBB11_16
	LONG $0x4f10fac5; BYTE $0x18   // vmovss	24(%rdi), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e5cf2c5; BYTE $0x18   // vsubss	24(%rsi), %xmm1, %xmm1
	LONG $0xc959f2c5               // vmulss	%xmm1, %xmm1, %xmm1
	LONG $0xc058f2c5               // vaddss	%xmm0, %xmm1, %xmm0

LBB11_16:
	LONG  $0xc051fac5         // vsqrtss	%xmm0, %xmm0, %xmm0
	WORD  $0x8948; BYTE $0xec // movq	%rbp, %rsp
	BYTE  $0x5d               // popq	%rbp
	WORD  $0xf8c5; BYTE $0x77 // vzeroupper
	MOVSS X0, result+24(FP)
	RET

TEXT ·_mm256_mm(SB), $0-64
	MOVQ  a+0(FP), DI
	MOVQ  b+8(FP), SI
	MOVQ  c+16(FP), DX
	MOVQ  m+24(FP), CX
	MOVQ  n+32(FP), R8
	MOVQ  k+40(FP), R9
	PUSHQ transB+49(FP)
	PUSHQ transA+48(FP)
	PUSHQ $0
	BYTE  $0x55                                 // pushq	%rbp
	WORD  $0x8948; BYTE $0xe5                   // movq	%rsp, %rbp
	WORD  $0x5741                               // pushq	%r15
	WORD  $0x5641                               // pushq	%r14
	WORD  $0x5541                               // pushq	%r13
	WORD  $0x5441                               // pushq	%r12
	BYTE  $0x53                                 // pushq	%rbx
	LONG  $0xf0e48348                           // andq	$-16, %rsp
	LONG  $0x90ec8148; WORD $0x0000; BYTE $0x00 // subq	$144, %rsp
	WORD  $0x8949; BYTE $0xd5                   // movq	%rdx, %r13
	LONG  $0x24748948; BYTE $0x10               // movq	%rsi, 16(%rsp)                  # 8-byte Spill
	LONG  $0x247c8948; BYTE $0x20               // movq	%rdi, 32(%rsp)                  # 8-byte Spill
	LONG  $0x55b60f44; BYTE $0x18               // movzbl	24(%rbp), %r10d
	LONG  $0x1045b60f                           // movzbl	16(%rbp), %eax
	WORD  $0xc289                               // movl	%eax, %edx
	WORD  $0x0844; BYTE $0xd2                   // orb	%r10b, %dl
	LONG  $0x246c894c; BYTE $0x70               // movq	%r13, 112(%rsp)                 # 8-byte Spill
	LONG  $0x244c8948; BYTE $0x28               // movq	%rcx, 40(%rsp)                  # 8-byte Spill
	LONG  $0x244c894c; BYTE $0x08               // movq	%r9, 8(%rsp)                    # 8-byte Spill
	JE    LBB12_1
	WORD  $0x8944; BYTE $0xd2                   // movl	%r10d, %edx
	WORD  $0xf280; BYTE $0x01                   // xorb	$1, %dl
	WORD  $0xc208                               // orb	%al, %dl
	JE    LBB12_16
	WORD  $0xc689                               // movl	%eax, %esi
	LONG  $0x01f68040                           // xorb	$1, %sil
	WORD  $0x0844; BYTE $0xd6                   // orb	%r10b, %sil
	LONG  $0x247c8348; WORD $0x0008             // cmpq	$0, 8(%rsp)                     # 8-byte Folded Reload
	LONG  $0xc79f0f40                           // setg	%dil
	WORD  $0x854d; BYTE $0xc0                   // testq	%r8, %r8
	WORD  $0x9f0f; BYTE $0xc2                   // setg	%dl
	WORD  $0x2040; BYTE $0xfa                   // andb	%dil, %dl
	WORD  $0x8440; BYTE $0xf6                   // testb	%sil, %sil
	JE    LBB12_81
	WORD  $0x8548; BYTE $0xc9                   // testq	%rcx, %rcx
	LONG  $0xc69f0f40                           // setg	%sil
	WORD  $0x2044; BYTE $0xd2                   // andb	%r10b, %dl
	WORD  $0x2040; BYTE $0xc6                   // andb	%al, %sil
	WORD  $0x2040; BYTE $0xd6                   // andb	%dl, %sil
	LONG  $0x01fe8040                           // cmpb	$1, %sil
	JNE   LBB12_110
	WORD  $0x8948; BYTE $0xcf                   // movq	%rcx, %rdi
	QUAD  $0x00000000850c8d4e                   // leaq	(,%r8,4), %r9
	LONG  $0x24548b48; BYTE $0x08               // movq	8(%rsp), %rdx                   # 8-byte Reload
	LONG  $0xff428d48                           // leaq	-1(%rdx), %rax
	LONG  $0xc1af0f48                           // imulq	%rcx, %rax
	LONG  $0x020c8d4a                           // leaq	(%rdx,%r8), %rcx
	LONG  $0x24748b48; BYTE $0x10               // movq	16(%rsp), %rsi                  # 8-byte Reload
	LONG  $0x8e0c8d48                           // leaq	(%rsi,%rcx,4), %rcx
	LONG  $0xfcc18348                           // addq	$-4, %rcx
	LONG  $0x244c8948; BYTE $0x40               // movq	%rcx, 64(%rsp)                  # 8-byte Spill
	LONG  $0x854c8d4b; BYTE $0x00               // leaq	(%r13,%r8,4), %rcx
	LONG  $0x244c8948; BYTE $0x38               // movq	%rcx, 56(%rsp)                  # 8-byte Spill
	LONG  $0x244c8b48; BYTE $0x20               // movq	32(%rsp), %rcx                  # 8-byte Reload
	LONG  $0x81048d48                           // leaq	(%rcx,%rax,4), %rax
	LONG  $0x04c08348                           // addq	$4, %rax
	LONG  $0x24448948; BYTE $0x78               // movq	%rax, 120(%rsp)                 # 8-byte Spill
	LONG  $0x20f88349                           // cmpq	$32, %r8
	WORD  $0x930f; BYTE $0xc0                   // setae	%al
	LONG  $0x01fa8348                           // cmpq	$1, %rdx
	WORD  $0x940f; BYTE $0xc1                   // sete	%cl
	WORD  $0xc120                               // andb	%al, %cl
	QUAD  $0xffffffffffe0bc49; WORD $0x7fff     // movabsq	$9223372036854775776, %r12      # imm = 0x7FFFFFFFFFFFFFE0
	WORD  $0x214d; BYTE $0xc4                   // andq	%r8, %r12
	LONG  $0xff408d49                           // leaq	-1(%r8), %rax
	LONG  $0x24448948; BYTE $0x68               // movq	%rax, 104(%rsp)                 # 8-byte Spill
	LONG  $0x60468d48                           // leaq	96(%rsi), %rax
	QUAD  $0x0000008024848948                   // movq	%rax, 128(%rsp)                 # 8-byte Spill
	LONG  $0x605d8d4d                           // leaq	96(%r13), %r11
	QUAD  $0x0000000095048d48                   // leaq	(,%rdx,4), %rax
	LONG  $0x24448948; BYTE $0x58               // movq	%rax, 88(%rsp)                  # 8-byte Spill
	QUAD  $0x00000000d5348d4c                   // leaq	(,%rdx,8), %r14
	LONG  $0x04458d49                           // leaq	4(%r13), %rax
	WORD  $0xd1f6                               // notb	%cl
	LONG  $0x1f244c88                           // movb	%cl, 31(%rsp)                   # 1-byte Spill
	WORD  $0x3145; BYTE $0xd2                   // xorl	%r10d, %r10d
	LONG  $0x244c894c; BYTE $0x48               // movq	%r9, 72(%rsp)                   # 8-byte Spill
	JMP   LBB12_97

LBB12_109:
	LONG $0x24548b4c; BYTE $0x50 // movq	80(%rsp), %r10                  # 8-byte Reload
	WORD $0xff49; BYTE $0xc2     // incq	%r10
	LONG $0x244c8b4c; BYTE $0x48 // movq	72(%rsp), %r9                   # 8-byte Reload
	WORD $0x014d; BYTE $0xcb     // addq	%r9, %r11
	WORD $0x014c; BYTE $0xc8     // addq	%r9, %rax
	LONG $0x247c8b48; BYTE $0x28 // movq	40(%rsp), %rdi                  # 8-byte Reload
	WORD $0x3949; BYTE $0xfa     // cmpq	%rdi, %r10
	LONG $0x246c8b4c; BYTE $0x70 // movq	112(%rsp), %r13                 # 8-byte Reload
	JE   LBB12_110

LBB12_97:
	QUAD $0x000000000000b948; WORD $0x2000 // movabsq	$2305843009213693952, %rcx      # imm = 0x2000000000000000
	WORD $0x8548; BYTE $0xcf               // testq	%rcx, %rdi
	WORD $0x950f; BYTE $0xc1               // setne	%cl
	WORD $0x894c; BYTE $0xce               // movq	%r9, %rsi
	LONG $0xf2af0f49                       // imulq	%r10, %rsi
	LONG $0x2e3c8d4a                       // leaq	(%rsi,%r13), %rdi
	LONG $0x24740348; BYTE $0x38           // addq	56(%rsp), %rsi                  # 8-byte Folded Reload
	LONG $0x24548b48; BYTE $0x20           // movq	32(%rsp), %rdx                  # 8-byte Reload
	LONG $0x92148d4a                       // leaq	(%rdx,%r10,4), %rdx
	LONG $0x244c8b4c; BYTE $0x78           // movq	120(%rsp), %r9                  # 8-byte Reload
	LONG $0x910c8d4f                       // leaq	(%r9,%r10,4), %r9
	LONG $0x2454894c; BYTE $0x50           // movq	%r10, 80(%rsp)                  # 8-byte Spill
	LONG $0xd0af0f4d                       // imulq	%r8, %r10
	QUAD $0x0000000095148d4e               // leaq	(,%r10,4), %r10
	WORD $0x014d; BYTE $0xea               // addq	%r13, %r10
	LONG $0x2454894c; BYTE $0x60           // movq	%r10, 96(%rsp)                  # 8-byte Spill
	WORD $0x394c; BYTE $0xcf               // cmpq	%r9, %rdi
	LONG $0xc1920f41                       // setb	%r9b
	WORD $0x3948; BYTE $0xf2               // cmpq	%rsi, %rdx
	LONG $0xc2920f41                       // setb	%r10b
	WORD $0x2045; BYTE $0xca               // andb	%r9b, %r10b
	WORD $0x0841; BYTE $0xca               // orb	%cl, %r10b
	LONG $0x247c3b48; BYTE $0x40           // cmpq	64(%rsp), %rdi                  # 8-byte Folded Reload
	WORD $0x920f; BYTE $0xc1               // setb	%cl
	LONG $0x247c8b48; BYTE $0x10           // movq	16(%rsp), %rdi                  # 8-byte Reload
	WORD $0x3948; BYTE $0xfe               // cmpq	%rdi, %rsi
	LONG $0xc6970f40                       // seta	%sil
	WORD $0x2040; BYTE $0xce               // andb	%cl, %sil
	WORD $0x0844; BYTE $0xd6               // orb	%r10b, %sil
	LONG $0x24740a40; BYTE $0x1f           // orb	31(%rsp), %sil                  # 1-byte Folded Reload
	LONG $0x24748840; BYTE $0x30           // movb	%sil, 48(%rsp)                  # 1-byte Spill
	WORD $0x8949; BYTE $0xfa               // movq	%rdi, %r10
	QUAD $0x0000008024bc8b4c               // movq	128(%rsp), %r15                 # 8-byte Reload
	WORD $0x3145; BYTE $0xed               // xorl	%r13d, %r13d
	JMP  LBB12_98

LBB12_108:
	WORD $0xff49; BYTE $0xc5     // incq	%r13
	LONG $0x04c78349             // addq	$4, %r15
	LONG $0x04c28349             // addq	$4, %r10
	LONG $0x246c3b4c; BYTE $0x08 // cmpq	8(%rsp), %r13                   # 8-byte Folded Reload
	JE   LBB12_109

LBB12_98:
	WORD $0x894c; BYTE $0xe9       // movq	%r13, %rcx
	LONG $0x4caf0f48; WORD $0x2824 // imulq	40(%rsp), %rcx                  # 8-byte Folded Reload
	LONG $0x302444f6; BYTE $0x01   // testb	$1, 48(%rsp)                    # 1-byte Folded Reload
	JE   LBB12_100
	WORD $0x3145; BYTE $0xc9       // xorl	%r9d, %r9d
	JMP  LBB12_103

LBB12_100:
	LONG $0x187de2c4; WORD $0x8a04 // vbroadcastss	(%rdx,%rcx,4), %ymm0
	WORD $0xf631                   // xorl	%esi, %esi

LBB12_101:
	LONG $0x597cc1c4; WORD $0xb74c; BYTE $0xa0 // vmulps	-96(%r15,%rsi,4), %ymm0, %ymm1
	LONG $0x5874c1c4; WORD $0xb34c; BYTE $0xa0 // vaddps	-96(%r11,%rsi,4), %ymm1, %ymm1
	LONG $0x597cc1c4; WORD $0xb754; BYTE $0xc0 // vmulps	-64(%r15,%rsi,4), %ymm0, %ymm2
	LONG $0x586cc1c4; WORD $0xb354; BYTE $0xc0 // vaddps	-64(%r11,%rsi,4), %ymm2, %ymm2
	LONG $0x597cc1c4; WORD $0xb75c; BYTE $0xe0 // vmulps	-32(%r15,%rsi,4), %ymm0, %ymm3
	LONG $0x5864c1c4; WORD $0xb35c; BYTE $0xe0 // vaddps	-32(%r11,%rsi,4), %ymm3, %ymm3
	LONG $0x597cc1c4; WORD $0xb724             // vmulps	(%r15,%rsi,4), %ymm0, %ymm4
	LONG $0x585cc1c4; WORD $0xb324             // vaddps	(%r11,%rsi,4), %ymm4, %ymm4
	LONG $0x117cc1c4; WORD $0xb34c; BYTE $0xa0 // vmovups	%ymm1, -96(%r11,%rsi,4)
	LONG $0x117cc1c4; WORD $0xb354; BYTE $0xc0 // vmovups	%ymm2, -64(%r11,%rsi,4)
	LONG $0x117cc1c4; WORD $0xb35c; BYTE $0xe0 // vmovups	%ymm3, -32(%r11,%rsi,4)
	LONG $0x117cc1c4; WORD $0xb324             // vmovups	%ymm4, (%r11,%rsi,4)
	LONG $0x20c68348                           // addq	$32, %rsi
	WORD $0x3949; BYTE $0xf4                   // cmpq	%rsi, %r12
	JNE  LBB12_101
	WORD $0x894d; BYTE $0xe1                   // movq	%r12, %r9
	WORD $0x394d; BYTE $0xc4                   // cmpq	%r8, %r12
	JE   LBB12_108

LBB12_103:
	WORD $0x894c; BYTE $0xcb       // movq	%r9, %rbx
	LONG $0x01c0f641               // testb	$1, %r8b
	JE   LBB12_105
	LONG $0x24748b48; BYTE $0x10   // movq	16(%rsp), %rsi                  # 8-byte Reload
	LONG $0xae348d4a               // leaq	(%rsi,%r13,4), %rsi
	LONG $0x0410fac5; BYTE $0x8a   // vmovss	(%rdx,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	WORD $0x894c; BYTE $0xcf       // movq	%r9, %rdi
	LONG $0x7caf0f48; WORD $0x0824 // imulq	8(%rsp), %rdi                   # 8-byte Folded Reload
	LONG $0x0459fac5; BYTE $0xbe   // vmulss	(%rsi,%rdi,4), %xmm0, %xmm0
	LONG $0x24748b48; BYTE $0x60   // movq	96(%rsp), %rsi                  # 8-byte Reload
	LONG $0x587aa1c4; WORD $0x8e04 // vaddss	(%rsi,%r9,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0x8e04 // vmovss	%xmm0, (%rsi,%r9,4)
	WORD $0x894c; BYTE $0xcb       // movq	%r9, %rbx
	LONG $0x01cb8348               // orq	$1, %rbx

LBB12_105:
	LONG $0x244c3b4c; BYTE $0x68 // cmpq	104(%rsp), %r9                  # 8-byte Folded Reload
	JE   LBB12_108
	LONG $0x24748b48; BYTE $0x58 // movq	88(%rsp), %rsi                  # 8-byte Reload
	WORD $0x8949; BYTE $0xf1     // movq	%rsi, %r9
	LONG $0xcbaf0f4c             // imulq	%rbx, %r9
	LONG $0x017b8d48             // leaq	1(%rbx), %rdi
	LONG $0xfeaf0f48             // imulq	%rsi, %rdi
	WORD $0x894c; BYTE $0xd6     // movq	%r10, %rsi

LBB12_107:
	LONG $0x0410fac5; BYTE $0x8a   // vmovss	(%rdx,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597aa1c4; WORD $0x0e04 // vmulss	(%rsi,%r9), %xmm0, %xmm0
	LONG $0x4458fac5; WORD $0xfc98 // vaddss	-4(%rax,%rbx,4), %xmm0, %xmm0
	LONG $0x4411fac5; WORD $0xfc98 // vmovss	%xmm0, -4(%rax,%rbx,4)
	LONG $0x0410fac5; BYTE $0x8a   // vmovss	(%rdx,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x0459fac5; BYTE $0x3e   // vmulss	(%rsi,%rdi), %xmm0, %xmm0
	LONG $0x0458fac5; BYTE $0x98   // vaddss	(%rax,%rbx,4), %xmm0, %xmm0
	LONG $0x0411fac5; BYTE $0x98   // vmovss	%xmm0, (%rax,%rbx,4)
	LONG $0x02c38348               // addq	$2, %rbx
	WORD $0x014c; BYTE $0xf6       // addq	%r14, %rsi
	WORD $0x3949; BYTE $0xd8       // cmpq	%rbx, %r8
	JNE  LBB12_107
	JMP  LBB12_108

LBB12_1:
	WORD $0x8548; BYTE $0xc9               // testq	%rcx, %rcx
	WORD $0x9e0f; BYTE $0xc0               // setle	%al
	WORD $0x854d; BYTE $0xc9               // testq	%r9, %r9
	WORD $0x9e0f; BYTE $0xc1               // setle	%cl
	WORD $0xc108                           // orb	%al, %cl
	WORD $0x854d; BYTE $0xc0               // testq	%r8, %r8
	WORD $0x9e0f; BYTE $0xc0               // setle	%al
	WORD $0xc808                           // orb	%cl, %al
	JNE  LBB12_110
	QUAD $0x0000000085048d4a               // leaq	(,%r8,4), %rax
	LONG $0x24548b48; BYTE $0x08           // movq	8(%rsp), %rdx                   # 8-byte Reload
	QUAD $0x00000000950c8d48               // leaq	(,%rdx,4), %rcx
	LONG $0x244c8948; BYTE $0x58           // movq	%rcx, 88(%rsp)                  # 8-byte Spill
	WORD $0x8948; BYTE $0xd1               // movq	%rdx, %rcx
	LONG $0xc8af0f49                       // imulq	%r8, %rcx
	LONG $0x24748b48; BYTE $0x10           // movq	16(%rsp), %rsi                  # 8-byte Reload
	LONG $0x8e0c8d48                       // leaq	(%rsi,%rcx,4), %rcx
	LONG $0x244c8948; BYTE $0x50           // movq	%rcx, 80(%rsp)                  # 8-byte Spill
	QUAD $0x00000000850c8d4a               // leaq	(,%r8,4), %rcx
	WORD $0x014c; BYTE $0xe9               // addq	%r13, %rcx
	LONG $0x244c8948; BYTE $0x48           // movq	%rcx, 72(%rsp)                  # 8-byte Spill
	LONG $0x244c8b48; BYTE $0x20           // movq	32(%rsp), %rcx                  # 8-byte Reload
	LONG $0x910c8d48                       // leaq	(%rcx,%rdx,4), %rcx
	LONG $0x244c8948; BYTE $0x40           // movq	%rcx, 64(%rsp)                  # 8-byte Spill
	QUAD $0xffffffffffe0bf49; WORD $0x7fff // movabsq	$9223372036854775776, %r15      # imm = 0x7FFFFFFFFFFFFFE0
	WORD $0x214d; BYTE $0xc7               // andq	%r8, %r15
	LONG $0xff488d49                       // leaq	-1(%r8), %rcx
	LONG $0x244c8948; BYTE $0x30           // movq	%rcx, 48(%rsp)                  # 8-byte Spill
	LONG $0x604e8d48                       // leaq	96(%rsi), %rcx
	LONG $0x244c8948; BYTE $0x38           // movq	%rcx, 56(%rsp)                  # 8-byte Spill
	LONG $0x60558d4d                       // leaq	96(%r13), %r10
	LONG $0x044e8d48                       // leaq	4(%rsi), %rcx
	LONG $0x244c8948; BYTE $0x78           // movq	%rcx, 120(%rsp)                 # 8-byte Spill
	LONG $0x045d8d49                       // leaq	4(%r13), %rbx
	WORD $0x3145; BYTE $0xf6               // xorl	%r14d, %r14d
	JMP  LBB12_3

LBB12_14:
	LONG $0x24748b4c; BYTE $0x68 // movq	104(%rsp), %r14                 # 8-byte Reload
	WORD $0xff49; BYTE $0xc6     // incq	%r14
	WORD $0x0149; BYTE $0xc2     // addq	%rax, %r10
	WORD $0x0148; BYTE $0xc3     // addq	%rax, %rbx
	LONG $0x24743b4c; BYTE $0x28 // cmpq	40(%rsp), %r14                  # 8-byte Folded Reload
	LONG $0x246c8b4c; BYTE $0x70 // movq	112(%rsp), %r13                 # 8-byte Reload
	JE   LBB12_110

LBB12_3:
	WORD $0x8548; BYTE $0xc0       // testq	%rax, %rax
	LONG $0x2444980f; BYTE $0x60   // sets	96(%rsp)                        # 1-byte Folded Spill
	WORD $0x8948; BYTE $0xc6       // movq	%rax, %rsi
	LONG $0xf6af0f49               // imulq	%r14, %rsi
	LONG $0x2e0c8d4e               // leaq	(%rsi,%r13), %r9
	LONG $0x24740348; BYTE $0x48   // addq	72(%rsp), %rsi                  # 8-byte Folded Reload
	LONG $0x24548b48; BYTE $0x58   // movq	88(%rsp), %rdx                  # 8-byte Reload
	LONG $0xd6af0f49               // imulq	%r14, %rdx
	LONG $0x24648b4c; BYTE $0x20   // movq	32(%rsp), %r12                  # 8-byte Reload
	LONG $0x143c8d49               // leaq	(%r12,%rdx), %rdi
	LONG $0x24540348; BYTE $0x40   // addq	64(%rsp), %rdx                  # 8-byte Folded Reload
	WORD $0x894d; BYTE $0xf3       // movq	%r14, %r11
	LONG $0x5caf0f4c; WORD $0x0824 // imulq	8(%rsp), %r11                   # 8-byte Folded Reload
	LONG $0x2474894c; BYTE $0x68   // movq	%r14, 104(%rsp)                 # 8-byte Spill
	LONG $0xf0af0f4d               // imulq	%r8, %r14
	WORD $0x3949; BYTE $0xd1       // cmpq	%rdx, %r9
	WORD $0x920f; BYTE $0xc1       // setb	%cl
	WORD $0x3948; BYTE $0xf7       // cmpq	%rsi, %rdi
	LONG $0x9c148d4b               // leaq	(%r12,%r11,4), %rdx
	LONG $0xb57c8d4b; BYTE $0x00   // leaq	(%r13,%r14,4), %rdi
	LONG $0xc5920f41               // setb	%r13b
	WORD $0x2041; BYTE $0xcd       // andb	%cl, %r13b
	LONG $0x244c3b4c; BYTE $0x50   // cmpq	80(%rsp), %r9                   # 8-byte Folded Reload
	WORD $0x920f; BYTE $0xc1       // setb	%cl
	LONG $0x24743b48; BYTE $0x10   // cmpq	16(%rsp), %rsi                  # 8-byte Folded Reload
	LONG $0xc6970f40               // seta	%sil
	WORD $0x2040; BYTE $0xce       // andb	%cl, %sil
	LONG $0x246c0a44; BYTE $0x60   // orb	96(%rsp), %r13b                 # 1-byte Folded Reload
	WORD $0x0841; BYTE $0xf5       // orb	%sil, %r13b
	LONG $0x244c8b48; BYTE $0x78   // movq	120(%rsp), %rcx                 # 8-byte Reload
	LONG $0x24748b48; BYTE $0x38   // movq	56(%rsp), %rsi                  # 8-byte Reload
	WORD $0x3145; BYTE $0xdb       // xorl	%r11d, %r11d
	JMP  LBB12_4

LBB12_13:
	WORD $0xff49; BYTE $0xc3     // incq	%r11
	WORD $0x0148; BYTE $0xc6     // addq	%rax, %rsi
	WORD $0x0148; BYTE $0xc1     // addq	%rax, %rcx
	LONG $0x245c3b4c; BYTE $0x08 // cmpq	8(%rsp), %r11                   # 8-byte Folded Reload
	JE   LBB12_14

LBB12_4:
	LONG $0x20f88349         // cmpq	$32, %r8
	LONG $0xc1920f41         // setb	%r9b
	WORD $0x0845; BYTE $0xe9 // orb	%r13b, %r9b
	JE   LBB12_6
	WORD $0x3145; BYTE $0xc9 // xorl	%r9d, %r9d
	JMP  LBB12_9

LBB12_6:
	LONG $0x187da2c4; WORD $0x9a04 // vbroadcastss	(%rdx,%r11,4), %ymm0
	WORD $0x3145; BYTE $0xc9       // xorl	%r9d, %r9d

LBB12_7:
	LONG $0x597ca1c4; WORD $0x8e4c; BYTE $0xa0 // vmulps	-96(%rsi,%r9,4), %ymm0, %ymm1
	LONG $0x587481c4; WORD $0x8a4c; BYTE $0xa0 // vaddps	-96(%r10,%r9,4), %ymm1, %ymm1
	LONG $0x597ca1c4; WORD $0x8e54; BYTE $0xc0 // vmulps	-64(%rsi,%r9,4), %ymm0, %ymm2
	LONG $0x586c81c4; WORD $0x8a54; BYTE $0xc0 // vaddps	-64(%r10,%r9,4), %ymm2, %ymm2
	LONG $0x597ca1c4; WORD $0x8e5c; BYTE $0xe0 // vmulps	-32(%rsi,%r9,4), %ymm0, %ymm3
	LONG $0x586481c4; WORD $0x8a5c; BYTE $0xe0 // vaddps	-32(%r10,%r9,4), %ymm3, %ymm3
	LONG $0x597ca1c4; WORD $0x8e24             // vmulps	(%rsi,%r9,4), %ymm0, %ymm4
	LONG $0x585c81c4; WORD $0x8a24             // vaddps	(%r10,%r9,4), %ymm4, %ymm4
	LONG $0x117c81c4; WORD $0x8a4c; BYTE $0xa0 // vmovups	%ymm1, -96(%r10,%r9,4)
	LONG $0x117c81c4; WORD $0x8a54; BYTE $0xc0 // vmovups	%ymm2, -64(%r10,%r9,4)
	LONG $0x117c81c4; WORD $0x8a5c; BYTE $0xe0 // vmovups	%ymm3, -32(%r10,%r9,4)
	LONG $0x117c81c4; WORD $0x8a24             // vmovups	%ymm4, (%r10,%r9,4)
	LONG $0x20c18349                           // addq	$32, %r9
	WORD $0x394d; BYTE $0xcf                   // cmpq	%r9, %r15
	JNE  LBB12_7
	WORD $0x894d; BYTE $0xf9                   // movq	%r15, %r9
	WORD $0x394d; BYTE $0xc7                   // cmpq	%r8, %r15
	JE   LBB12_13

LBB12_9:
	WORD $0x894d; BYTE $0xce       // movq	%r9, %r14
	LONG $0x01c0f641               // testb	$1, %r8b
	JE   LBB12_11
	WORD $0x894d; BYTE $0xde       // movq	%r11, %r14
	LONG $0xf0af0f4d               // imulq	%r8, %r14
	LONG $0x24648b4c; BYTE $0x10   // movq	16(%rsp), %r12                  # 8-byte Reload
	LONG $0xb4348d4f               // leaq	(%r12,%r14,4), %r14
	LONG $0x107aa1c4; WORD $0x9a04 // vmovss	(%rdx,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597a81c4; WORD $0x8e04 // vmulss	(%r14,%r9,4), %xmm0, %xmm0
	LONG $0x587aa1c4; WORD $0x8f04 // vaddss	(%rdi,%r9,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0x8f04 // vmovss	%xmm0, (%rdi,%r9,4)
	WORD $0x894d; BYTE $0xce       // movq	%r9, %r14
	LONG $0x01ce8349               // orq	$1, %r14

LBB12_11:
	LONG $0x244c3b4c; BYTE $0x30 // cmpq	48(%rsp), %r9                   # 8-byte Folded Reload
	JE   LBB12_13

LBB12_12:
	LONG $0x107aa1c4; WORD $0x9a04             // vmovss	(%rdx,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597aa1c4; WORD $0xb144; BYTE $0xfc // vmulss	-4(%rcx,%r14,4), %xmm0, %xmm0
	LONG $0x587aa1c4; WORD $0xb344; BYTE $0xfc // vaddss	-4(%rbx,%r14,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0xb344; BYTE $0xfc // vmovss	%xmm0, -4(%rbx,%r14,4)
	LONG $0x107aa1c4; WORD $0x9a04             // vmovss	(%rdx,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597aa1c4; WORD $0xb104             // vmulss	(%rcx,%r14,4), %xmm0, %xmm0
	LONG $0x587aa1c4; WORD $0xb304             // vaddss	(%rbx,%r14,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0xb304             // vmovss	%xmm0, (%rbx,%r14,4)
	LONG $0x02c68349                           // addq	$2, %r14
	WORD $0x394d; BYTE $0xf0                   // cmpq	%r14, %r8
	JNE  LBB12_12
	JMP  LBB12_13

LBB12_16:
	WORD $0x8548; BYTE $0xc9     // testq	%rcx, %rcx
	JLE  LBB12_110
	LONG $0x24448b48; BYTE $0x08 // movq	8(%rsp), %rax                   # 8-byte Reload
	LONG $0x07488d48             // leaq	7(%rax), %rcx
	WORD $0x8548; BYTE $0xc0     // testq	%rax, %rax
	LONG $0xc8490f48             // cmovnsq	%rax, %rcx
	WORD $0x854d; BYTE $0xc0     // testq	%r8, %r8
	JLE  LBB12_110
	WORD $0x8948; BYTE $0xca     // movq	%rcx, %rdx
	LONG $0xf8e28348             // andq	$-8, %rdx
	LONG $0x24748b48; BYTE $0x08 // movq	8(%rsp), %rsi                   # 8-byte Reload
	WORD $0x8948; BYTE $0xf0     // movq	%rsi, %rax
	WORD $0x2948; BYTE $0xd0     // subq	%rdx, %rax
	LONG $0x03f9c148             // sarq	$3, %rcx
	WORD $0xf983; BYTE $0x02     // cmpl	$2, %ecx
	JL   LBB12_47
	WORD $0x8548; BYTE $0xc0     // testq	%rax, %rax
	JLE  LBB12_20
	WORD $0xc089                 // movl	%eax, %eax
	LONG $0xff518d44             // leal	-1(%rcx), %r10d
	WORD $0x798d; BYTE $0xfe     // leal	-2(%rcx), %edi
	WORD $0x8944; BYTE $0xd3     // movl	%r10d, %ebx
	WORD $0xe383; BYTE $0x03     // andl	$3, %ebx
	LONG $0xfce28341             // andl	$-4, %r10d
	WORD $0x3145; BYTE $0xc9     // xorl	%r9d, %r9d
	LONG $0x24748b48; BYTE $0x08 // movq	8(%rsp), %rsi                   # 8-byte Reload
	JMP  LBB12_25

LBB12_39:
	LONG $0x244c8b4c; BYTE $0x30 // movq	48(%rsp), %r9                   # 8-byte Reload
	WORD $0xff49; BYTE $0xc1     // incq	%r9
	LONG $0x244c3b4c; BYTE $0x28 // cmpq	40(%rsp), %r9                   # 8-byte Folded Reload
	LONG $0x246c8b4c; BYTE $0x70 // movq	112(%rsp), %r13                 # 8-byte Reload
	JE   LBB12_110

LBB12_25:
	WORD $0x894c; BYTE $0xc9     // movq	%r9, %rcx
	LONG $0xceaf0f48             // imulq	%rsi, %rcx
	LONG $0x24548b48; BYTE $0x20 // movq	32(%rsp), %rdx                  # 8-byte Reload
	LONG $0x8a3c8d4c             // leaq	(%rdx,%rcx,4), %r15
	LONG $0x8a148d48             // leaq	(%rdx,%rcx,4), %rdx
	LONG $0x20c28348             // addq	$32, %rdx
	LONG $0x244c894c; BYTE $0x30 // movq	%r9, 48(%rsp)                   # 8-byte Spill
	WORD $0x894c; BYTE $0xc9     // movq	%r9, %rcx
	LONG $0xc8af0f49             // imulq	%r8, %rcx
	LONG $0x8d6c8d4d; BYTE $0x00 // leaq	(%r13,%rcx,4), %r13
	WORD $0x3145; BYTE $0xf6     // xorl	%r14d, %r14d
	JMP  LBB12_26

LBB12_38:
	LONG $0x117a81c4; WORD $0xb544; BYTE $0x00 // vmovss	%xmm0, (%r13,%r14,4)
	WORD $0xff49; BYTE $0xc6                   // incq	%r14
	WORD $0x394d; BYTE $0xc6                   // cmpq	%r8, %r14
	JE   LBB12_39

LBB12_26:
	WORD $0x894c; BYTE $0xf1       // movq	%r14, %rcx
	LONG $0xceaf0f48               // imulq	%rsi, %rcx
	LONG $0x107cc1c4; BYTE $0x07   // vmovups	(%r15), %ymm0
	LONG $0x244c8b4c; BYTE $0x10   // movq	16(%rsp), %r9                   # 8-byte Reload
	LONG $0x597cc1c4; WORD $0x8904 // vmulps	(%r9,%rcx,4), %ymm0, %ymm0
	LONG $0x891c8d4d               // leaq	(%r9,%rcx,4), %r11
	LONG $0x20c38349               // addq	$32, %r11
	WORD $0x8944; BYTE $0xd1       // movl	%r10d, %ecx
	WORD $0x8949; BYTE $0xd4       // movq	%rdx, %r12
	WORD $0xff83; BYTE $0x03       // cmpl	$3, %edi
	JB   LBB12_28

LBB12_27:
	LONG $0x107cc1c4; WORD $0x240c             // vmovups	(%r12), %ymm1
	LONG $0x107cc1c4; WORD $0x2454; BYTE $0x20 // vmovups	32(%r12), %ymm2
	LONG $0x107cc1c4; WORD $0x245c; BYTE $0x40 // vmovups	64(%r12), %ymm3
	LONG $0x107cc1c4; WORD $0x2464; BYTE $0x60 // vmovups	96(%r12), %ymm4
	LONG $0x5974c1c4; BYTE $0x0b               // vmulps	(%r11), %ymm1, %ymm1
	LONG $0xc158fcc5                           // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x596cc1c4; WORD $0x204b             // vmulps	32(%r11), %ymm2, %ymm1
	LONG $0x5964c1c4; WORD $0x4053             // vmulps	64(%r11), %ymm3, %ymm2
	LONG $0xc158fcc5                           // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0xc258fcc5                           // vaddps	%ymm2, %ymm0, %ymm0
	LONG $0x595cc1c4; WORD $0x604b             // vmulps	96(%r11), %ymm4, %ymm1
	LONG $0xc158fcc5                           // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x80ec8349                           // subq	$-128, %r12
	LONG $0x80eb8349                           // subq	$-128, %r11
	WORD $0xc183; BYTE $0xfc                   // addl	$-4, %ecx
	JNE  LBB12_27

LBB12_28:
	WORD $0xdb85  // testl	%ebx, %ebx
	JE   LBB12_31
	WORD $0xd989  // movl	%ebx, %ecx

LBB12_30:
	LONG $0x107cc1c4; WORD $0x240c // vmovups	(%r12), %ymm1
	LONG $0x5974c1c4; BYTE $0x0b   // vmulps	(%r11), %ymm1, %ymm1
	LONG $0xc158fcc5               // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x20c48349               // addq	$32, %r12
	LONG $0x20c38349               // addq	$32, %r11
	WORD $0xc9ff                   // decl	%ecx
	JNE  LBB12_30

LBB12_31:
	LONG $0x197de3c4; WORD $0x01c1             // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5                           // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01               // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5                           // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5                           // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5                           // vaddss	%xmm1, %xmm0, %xmm0
	LONG $0x107ac1c4; WORD $0x240c             // vmovss	(%r12), %xmm1                   # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; BYTE $0x0b               // vmulss	(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x01                   // cmpl	$1, %eax
	JE   LBB12_38
	LONG $0x107ac1c4; WORD $0x244c; BYTE $0x04 // vmovss	4(%r12), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0x044b             // vmulss	4(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x02                   // cmpl	$2, %eax
	JE   LBB12_38
	LONG $0x107ac1c4; WORD $0x244c; BYTE $0x08 // vmovss	8(%r12), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0x084b             // vmulss	8(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x03                   // cmpl	$3, %eax
	JE   LBB12_38
	LONG $0x107ac1c4; WORD $0x244c; BYTE $0x0c // vmovss	12(%r12), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0x0c4b             // vmulss	12(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x04                   // cmpl	$4, %eax
	JE   LBB12_38
	LONG $0x107ac1c4; WORD $0x244c; BYTE $0x10 // vmovss	16(%r12), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0x104b             // vmulss	16(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x05                   // cmpl	$5, %eax
	JE   LBB12_38
	LONG $0x107ac1c4; WORD $0x244c; BYTE $0x14 // vmovss	20(%r12), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0x144b             // vmulss	20(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x06                   // cmpl	$6, %eax
	JE   LBB12_38
	LONG $0x107ac1c4; WORD $0x244c; BYTE $0x18 // vmovss	24(%r12), %xmm1                 # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0x184b             // vmulss	24(%r11), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	JMP  LBB12_38

LBB12_81:
	WORD $0x8548; BYTE $0xc9               // testq	%rcx, %rcx
	WORD $0x9e0f; BYTE $0xc0               // setle	%al
	WORD $0xf280; BYTE $0x01               // xorb	$1, %dl
	WORD $0xc208                           // orb	%al, %dl
	JNE  LBB12_110
	WORD $0x8948; BYTE $0xce               // movq	%rcx, %rsi
	QUAD $0x0000000085148d4e               // leaq	(,%r8,4), %r10
	LONG $0x244c8b48; BYTE $0x08           // movq	8(%rsp), %rcx                   # 8-byte Reload
	LONG $0xff418d48                       // leaq	-1(%rcx), %rax
	LONG $0xc6af0f48                       // imulq	%rsi, %rax
	LONG $0xc8af0f49                       // imulq	%r8, %rcx
	LONG $0x24548b48; BYTE $0x10           // movq	16(%rsp), %rdx                  # 8-byte Reload
	LONG $0x8a0c8d48                       // leaq	(%rdx,%rcx,4), %rcx
	LONG $0x244c8948; BYTE $0x58           // movq	%rcx, 88(%rsp)                  # 8-byte Spill
	QUAD $0x00000000850c8d4a               // leaq	(,%r8,4), %rcx
	WORD $0x014c; BYTE $0xe9               // addq	%r13, %rcx
	LONG $0x244c8948; BYTE $0x50           // movq	%rcx, 80(%rsp)                  # 8-byte Spill
	LONG $0x244c8b48; BYTE $0x20           // movq	32(%rsp), %rcx                  # 8-byte Reload
	LONG $0x81048d48                       // leaq	(%rcx,%rax,4), %rax
	LONG $0x04c08348                       // addq	$4, %rax
	LONG $0x24448948; BYTE $0x48           // movq	%rax, 72(%rsp)                  # 8-byte Spill
	QUAD $0xffffffffffe0bf49; WORD $0x7fff // movabsq	$9223372036854775776, %r15      # imm = 0x7FFFFFFFFFFFFFE0
	WORD $0x214d; BYTE $0xc7               // andq	%r8, %r15
	LONG $0xff408d49                       // leaq	-1(%r8), %rax
	LONG $0x24448948; BYTE $0x30           // movq	%rax, 48(%rsp)                  # 8-byte Spill
	LONG $0x60428d48                       // leaq	96(%rdx), %rax
	LONG $0x24448948; BYTE $0x40           // movq	%rax, 64(%rsp)                  # 8-byte Spill
	LONG $0x60458d49                       // leaq	96(%r13), %rax
	LONG $0x044a8d48                       // leaq	4(%rdx), %rcx
	LONG $0x244c8948; BYTE $0x38           // movq	%rcx, 56(%rsp)                  # 8-byte Spill
	LONG $0x045d8d49                       // leaq	4(%r13), %rbx
	WORD $0x3145; BYTE $0xf6               // xorl	%r14d, %r14d
	JMP  LBB12_83

LBB12_94:
	LONG $0x24748b4c; BYTE $0x60 // movq	96(%rsp), %r14                  # 8-byte Reload
	WORD $0xff49; BYTE $0xc6     // incq	%r14
	WORD $0x014c; BYTE $0xd0     // addq	%r10, %rax
	WORD $0x014c; BYTE $0xd3     // addq	%r10, %rbx
	LONG $0x24748b48; BYTE $0x28 // movq	40(%rsp), %rsi                  # 8-byte Reload
	WORD $0x3949; BYTE $0xf6     // cmpq	%rsi, %r14
	LONG $0x246c8b4c; BYTE $0x70 // movq	112(%rsp), %r13                 # 8-byte Reload
	JE   LBB12_110

LBB12_83:
	WORD $0x854d; BYTE $0xd2               // testq	%r10, %r10
	WORD $0x980f; BYTE $0xc1               // sets	%cl
	QUAD $0x000000000000ba48; WORD $0x2000 // movabsq	$2305843009213693952, %rdx      # imm = 0x2000000000000000
	WORD $0x8548; BYTE $0xd6               // testq	%rdx, %rsi
	LONG $0xc6950f40                       // setne	%sil
	WORD $0x894d; BYTE $0xd1               // movq	%r10, %r9
	LONG $0xceaf0f4d                       // imulq	%r14, %r9
	LONG $0x291c8d4f                       // leaq	(%r9,%r13), %r11
	LONG $0x244c034c; BYTE $0x50           // addq	80(%rsp), %r9                   # 8-byte Folded Reload
	LONG $0x24548b48; BYTE $0x20           // movq	32(%rsp), %rdx                  # 8-byte Reload
	LONG $0xb2148d4a                       // leaq	(%rdx,%r14,4), %rdx
	LONG $0x247c8b48; BYTE $0x48           // movq	72(%rsp), %rdi                  # 8-byte Reload
	LONG $0xb73c8d4a                       // leaq	(%rdi,%r14,4), %rdi
	LONG $0x2474894c; BYTE $0x60           // movq	%r14, 96(%rsp)                  # 8-byte Spill
	LONG $0xf0af0f4d                       // imulq	%r8, %r14
	WORD $0x3949; BYTE $0xfb               // cmpq	%rdi, %r11
	LONG $0xc7920f40                       // setb	%dil
	WORD $0x394c; BYTE $0xca               // cmpq	%r9, %rdx
	LONG $0xc4920f41                       // setb	%r12b
	WORD $0x2041; BYTE $0xfc               // andb	%dil, %r12b
	QUAD $0x00000000b53c8d4a               // leaq	(,%r14,4), %rdi
	WORD $0x014c; BYTE $0xef               // addq	%r13, %rdi
	LONG $0x247c8948; BYTE $0x68           // movq	%rdi, 104(%rsp)                 # 8-byte Spill
	WORD $0x0841; BYTE $0xf4               // orb	%sil, %r12b
	LONG $0x245c3b4c; BYTE $0x58           // cmpq	88(%rsp), %r11                  # 8-byte Folded Reload
	LONG $0xc6920f40                       // setb	%sil
	LONG $0x244c3b4c; BYTE $0x10           // cmpq	16(%rsp), %r9                   # 8-byte Folded Reload
	LONG $0xc5970f41                       // seta	%r13b
	WORD $0x2041; BYTE $0xf5               // andb	%sil, %r13b
	WORD $0x0841; BYTE $0xcd               // orb	%cl, %r13b
	WORD $0x0845; BYTE $0xe5               // orb	%r12b, %r13b
	LONG $0x244c8b48; BYTE $0x38           // movq	56(%rsp), %rcx                  # 8-byte Reload
	LONG $0x24748b48; BYTE $0x40           // movq	64(%rsp), %rsi                  # 8-byte Reload
	WORD $0x3145; BYTE $0xf6               // xorl	%r14d, %r14d
	JMP  LBB12_84

LBB12_93:
	WORD $0xff49; BYTE $0xc6     // incq	%r14
	WORD $0x014c; BYTE $0xd6     // addq	%r10, %rsi
	WORD $0x014c; BYTE $0xd1     // addq	%r10, %rcx
	LONG $0x24743b4c; BYTE $0x08 // cmpq	8(%rsp), %r14                   # 8-byte Folded Reload
	JE   LBB12_94

LBB12_84:
	LONG $0x20f88349               // cmpq	$32, %r8
	LONG $0xc1920f41               // setb	%r9b
	WORD $0x894d; BYTE $0xf3       // movq	%r14, %r11
	LONG $0x5caf0f4c; WORD $0x2824 // imulq	40(%rsp), %r11                  # 8-byte Folded Reload
	WORD $0x0845; BYTE $0xe9       // orb	%r13b, %r9b
	LONG $0x01c1f641               // testb	$1, %r9b
	JE   LBB12_86
	WORD $0x3145; BYTE $0xe4       // xorl	%r12d, %r12d
	JMP  LBB12_89

LBB12_86:
	LONG $0x187da2c4; WORD $0x9a04 // vbroadcastss	(%rdx,%r11,4), %ymm0
	WORD $0x3145; BYTE $0xc9       // xorl	%r9d, %r9d

LBB12_87:
	LONG $0x597ca1c4; WORD $0x8e4c; BYTE $0xa0 // vmulps	-96(%rsi,%r9,4), %ymm0, %ymm1
	LONG $0x5874a1c4; WORD $0x884c; BYTE $0xa0 // vaddps	-96(%rax,%r9,4), %ymm1, %ymm1
	LONG $0x597ca1c4; WORD $0x8e54; BYTE $0xc0 // vmulps	-64(%rsi,%r9,4), %ymm0, %ymm2
	LONG $0x586ca1c4; WORD $0x8854; BYTE $0xc0 // vaddps	-64(%rax,%r9,4), %ymm2, %ymm2
	LONG $0x597ca1c4; WORD $0x8e5c; BYTE $0xe0 // vmulps	-32(%rsi,%r9,4), %ymm0, %ymm3
	LONG $0x5864a1c4; WORD $0x885c; BYTE $0xe0 // vaddps	-32(%rax,%r9,4), %ymm3, %ymm3
	LONG $0x597ca1c4; WORD $0x8e24             // vmulps	(%rsi,%r9,4), %ymm0, %ymm4
	LONG $0x585ca1c4; WORD $0x8824             // vaddps	(%rax,%r9,4), %ymm4, %ymm4
	LONG $0x117ca1c4; WORD $0x884c; BYTE $0xa0 // vmovups	%ymm1, -96(%rax,%r9,4)
	LONG $0x117ca1c4; WORD $0x8854; BYTE $0xc0 // vmovups	%ymm2, -64(%rax,%r9,4)
	LONG $0x117ca1c4; WORD $0x885c; BYTE $0xe0 // vmovups	%ymm3, -32(%rax,%r9,4)
	LONG $0x117ca1c4; WORD $0x8824             // vmovups	%ymm4, (%rax,%r9,4)
	LONG $0x20c18349                           // addq	$32, %r9
	WORD $0x394d; BYTE $0xcf                   // cmpq	%r9, %r15
	JNE  LBB12_87
	WORD $0x894d; BYTE $0xfc                   // movq	%r15, %r12
	WORD $0x394d; BYTE $0xc7                   // cmpq	%r8, %r15
	JE   LBB12_93

LBB12_89:
	WORD $0x894d; BYTE $0xe1       // movq	%r12, %r9
	LONG $0x01c0f641               // testb	$1, %r8b
	JE   LBB12_91
	WORD $0x894d; BYTE $0xf1       // movq	%r14, %r9
	LONG $0xc8af0f4d               // imulq	%r8, %r9
	LONG $0x247c8b48; BYTE $0x10   // movq	16(%rsp), %rdi                  # 8-byte Reload
	LONG $0x8f0c8d4e               // leaq	(%rdi,%r9,4), %r9
	LONG $0x107aa1c4; WORD $0x9a04 // vmovss	(%rdx,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597a81c4; WORD $0xa104 // vmulss	(%r9,%r12,4), %xmm0, %xmm0
	LONG $0x247c8b48; BYTE $0x68   // movq	104(%rsp), %rdi                 # 8-byte Reload
	LONG $0x587aa1c4; WORD $0xa704 // vaddss	(%rdi,%r12,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0xa704 // vmovss	%xmm0, (%rdi,%r12,4)
	WORD $0x894d; BYTE $0xe1       // movq	%r12, %r9
	LONG $0x01c98349               // orq	$1, %r9

LBB12_91:
	LONG $0x24643b4c; BYTE $0x30 // cmpq	48(%rsp), %r12                  # 8-byte Folded Reload
	JE   LBB12_93

LBB12_92:
	LONG $0x107aa1c4; WORD $0x9a04             // vmovss	(%rdx,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597aa1c4; WORD $0x8944; BYTE $0xfc // vmulss	-4(%rcx,%r9,4), %xmm0, %xmm0
	LONG $0x587aa1c4; WORD $0x8b44; BYTE $0xfc // vaddss	-4(%rbx,%r9,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0x8b44; BYTE $0xfc // vmovss	%xmm0, -4(%rbx,%r9,4)
	LONG $0x107aa1c4; WORD $0x9a04             // vmovss	(%rdx,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	LONG $0x597aa1c4; WORD $0x8904             // vmulss	(%rcx,%r9,4), %xmm0, %xmm0
	LONG $0x587aa1c4; WORD $0x8b04             // vaddss	(%rbx,%r9,4), %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0x8b04             // vmovss	%xmm0, (%rbx,%r9,4)
	LONG $0x02c18349                           // addq	$2, %r9
	WORD $0x394d; BYTE $0xc8                   // cmpq	%r9, %r8
	JNE  LBB12_92
	JMP  LBB12_93

LBB12_47:
	WORD $0x8548; BYTE $0xc0       // testq	%rax, %rax
	LONG $0x244c8b4c; BYTE $0x28   // movq	40(%rsp), %r9                   # 8-byte Reload
	JLE  LBB12_71
	WORD $0xc089                   // movl	%eax, %eax
	WORD $0xc985                   // testl	%ecx, %ecx
	JLE  LBB12_49
	LONG $0x24448348; WORD $0x3810 // addq	$56, 16(%rsp)                   # 8-byte Folded Spill
	QUAD $0x00000000b5148d48       // leaq	(,%rsi,4), %rdx
	QUAD $0x00000000853c8d4a       // leaq	(,%r8,4), %rdi
	WORD $0x3145; BYTE $0xd2       // xorl	%r10d, %r10d
	LONG $0x245c8b48; BYTE $0x28   // movq	40(%rsp), %rbx                  # 8-byte Reload
	LONG $0x244c8b4c; BYTE $0x20   // movq	32(%rsp), %r9                   # 8-byte Reload
	JMP  LBB12_61

LBB12_70:
	WORD $0xff49; BYTE $0xc2 // incq	%r10
	WORD $0x0149; BYTE $0xfd // addq	%rdi, %r13
	WORD $0x3949; BYTE $0xda // cmpq	%rbx, %r10
	JE   LBB12_110

LBB12_61:
	WORD $0x894c; BYTE $0xd1       // movq	%r10, %rcx
	LONG $0x4caf0f48; WORD $0x0824 // imulq	8(%rsp), %rcx                   # 8-byte Folded Reload
	LONG $0x24748b48; BYTE $0x10   // movq	16(%rsp), %rsi                  # 8-byte Reload
	WORD $0x3145; BYTE $0xdb       // xorl	%r11d, %r11d
	JMP  LBB12_62

LBB12_69:
	LONG $0x117a81c4; WORD $0x9d44; BYTE $0x00 // vmovss	%xmm0, (%r13,%r11,4)
	WORD $0xff49; BYTE $0xc3                   // incq	%r11
	WORD $0x0148; BYTE $0xd6                   // addq	%rdx, %rsi
	WORD $0x394d; BYTE $0xd8                   // cmpq	%r11, %r8
	JE   LBB12_70

LBB12_62:
	LONG $0x107cc1c4; WORD $0x8904             // vmovups	(%r9,%rcx,4), %ymm0
	LONG $0x4659fcc5; BYTE $0xc8               // vmulps	-56(%rsi), %ymm0, %ymm0
	LONG $0x197de3c4; WORD $0x01c1             // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5                           // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01               // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5                           // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5                           // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5                           // vaddss	%xmm1, %xmm0, %xmm0
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x20 // vmovss	32(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0xe8               // vmulss	-24(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x01                   // cmpl	$1, %eax
	JE   LBB12_69
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x24 // vmovss	36(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0xec               // vmulss	-20(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x02                   // cmpl	$2, %eax
	JE   LBB12_69
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x28 // vmovss	40(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0xf0               // vmulss	-16(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x03                   // cmpl	$3, %eax
	JE   LBB12_69
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x2c // vmovss	44(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0xf4               // vmulss	-12(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x04                   // cmpl	$4, %eax
	JE   LBB12_69
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x30 // vmovss	48(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0xf8               // vmulss	-8(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x05                   // cmpl	$5, %eax
	JE   LBB12_69
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x34 // vmovss	52(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x4e59f2c5; BYTE $0xfc               // vmulss	-4(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	WORD $0xf883; BYTE $0x06                   // cmpl	$6, %eax
	JE   LBB12_69
	LONG $0x107ac1c4; WORD $0x894c; BYTE $0x38 // vmovss	56(%r9,%rcx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	LONG $0x0e59f2c5                           // vmulss	(%rsi), %xmm1, %xmm1
	LONG $0xc058f2c5                           // vaddss	%xmm0, %xmm1, %xmm0
	JMP  LBB12_69

LBB12_20:
	WORD $0x418d; BYTE $0xff // leal	-1(%rcx), %eax
	WORD $0x518d; BYTE $0xfe // leal	-2(%rcx), %edx
	WORD $0xc789             // movl	%eax, %edi
	WORD $0xe783; BYTE $0xfc // andl	$-4, %edi
	WORD $0xc9fe             // decb	%cl
	LONG $0xd1b60f44         // movzbl	%cl, %r10d
	LONG $0x03e28341         // andl	$3, %r10d
	LONG $0x05e2c141         // shll	$5, %r10d
	WORD $0x3145; BYTE $0xdb // xorl	%r11d, %r11d
	JMP  LBB12_21

LBB12_46:
	WORD $0xff49; BYTE $0xc3     // incq	%r11
	LONG $0x245c3b4c; BYTE $0x28 // cmpq	40(%rsp), %r11                  # 8-byte Folded Reload
	JE   LBB12_110

LBB12_21:
	WORD $0x894c; BYTE $0xd9       // movq	%r11, %rcx
	LONG $0x4caf0f48; WORD $0x0824 // imulq	8(%rsp), %rcx                   # 8-byte Folded Reload
	LONG $0x24748b48; BYTE $0x20   // movq	32(%rsp), %rsi                  # 8-byte Reload
	LONG $0x8e0c8d48               // leaq	(%rsi,%rcx,4), %rcx
	WORD $0x894c; BYTE $0xde       // movq	%r11, %rsi
	LONG $0xf0af0f49               // imulq	%r8, %rsi
	QUAD $0x00000000b5348d48       // leaq	(,%rsi,4), %rsi
	WORD $0x014c; BYTE $0xee       // addq	%r13, %rsi
	WORD $0xdb31                   // xorl	%ebx, %ebx
	JMP  LBB12_22

LBB12_45:
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5               // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01   // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5               // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5               // vaddss	%xmm1, %xmm0, %xmm0
	LONG $0x0411fac5; BYTE $0x9e   // vmovss	%xmm0, (%rsi,%rbx,4)
	WORD $0xff48; BYTE $0xc3       // incq	%rbx
	WORD $0x394c; BYTE $0xc3       // cmpq	%r8, %rbx
	JE   LBB12_46

LBB12_22:
	WORD $0x8949; BYTE $0xd9       // movq	%rbx, %r9
	LONG $0x4caf0f4c; WORD $0x0824 // imulq	8(%rsp), %r9                    # 8-byte Folded Reload
	LONG $0x247c8b4c; BYTE $0x10   // movq	16(%rsp), %r15                  # 8-byte Reload
	LONG $0x8f348d4f               // leaq	(%r15,%r9,4), %r14
	LONG $0x0110fcc5               // vmovups	(%rcx), %ymm0
	LONG $0x597c81c4; WORD $0x8f04 // vmulps	(%r15,%r9,4), %ymm0, %ymm0
	WORD $0xfa83; BYTE $0x03       // cmpl	$3, %edx
	JAE  LBB12_40
	WORD $0x8949; BYTE $0xcf       // movq	%rcx, %r15
	JMP  LBB12_42

LBB12_40:
	WORD $0x8941; BYTE $0xfc // movl	%edi, %r12d
	WORD $0x8949; BYTE $0xcf // movq	%rcx, %r15

LBB12_41:
	LONG $0x107cc1c4; WORD $0x204f       // vmovups	32(%r15), %ymm1
	LONG $0x107cc1c4; WORD $0x4057       // vmovups	64(%r15), %ymm2
	LONG $0x107cc1c4; WORD $0x605f       // vmovups	96(%r15), %ymm3
	QUAD $0x000080a7107cc1c4; BYTE $0x00 // vmovups	128(%r15), %ymm4
	LONG $0x5974c1c4; WORD $0x204e       // vmulps	32(%r14), %ymm1, %ymm1
	LONG $0xc158fcc5                     // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x596cc1c4; WORD $0x404e       // vmulps	64(%r14), %ymm2, %ymm1
	LONG $0x5964c1c4; WORD $0x6056       // vmulps	96(%r14), %ymm3, %ymm2
	LONG $0xc158fcc5                     // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0xc258fcc5                     // vaddps	%ymm2, %ymm0, %ymm0
	QUAD $0x0000808e595cc1c4; BYTE $0x00 // vmulps	128(%r14), %ymm4, %ymm1
	LONG $0x80ef8349                     // subq	$-128, %r15
	LONG $0x80ee8349                     // subq	$-128, %r14
	LONG $0xc158fcc5                     // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0xfcc48341                     // addl	$-4, %r12d
	JNE  LBB12_41

LBB12_42:
	WORD $0x03a8             // testb	$3, %al
	JE   LBB12_45
	WORD $0x3145; BYTE $0xc9 // xorl	%r9d, %r9d

LBB12_44:
	LONG $0x107c81c4; WORD $0x0f4c; BYTE $0x20 // vmovups	32(%r15,%r9), %ymm1
	LONG $0x597481c4; WORD $0x0e4c; BYTE $0x20 // vmulps	32(%r14,%r9), %ymm1, %ymm1
	LONG $0xc158fcc5                           // vaddps	%ymm1, %ymm0, %ymm0
	LONG $0x20c18349                           // addq	$32, %r9
	WORD $0x3945; BYTE $0xca                   // cmpl	%r9d, %r10d
	JNE  LBB12_44
	JMP  LBB12_45

LBB12_71:
	WORD $0xc985                           // testl	%ecx, %ecx
	JLE  LBB12_111
	QUAD $0xffffffffffe0b848; WORD $0x7fff // movabsq	$9223372036854775776, %rax      # imm = 0x7FFFFFFFFFFFFFE0
	LONG $0x1ec88348                       // orq	$30, %rax
	WORD $0x214c; BYTE $0xc0               // andq	%r8, %rax
	QUAD $0x00000000f50c8d48               // leaq	(,%rsi,8), %rcx
	LONG $0x04558d49                       // leaq	4(%r13), %rdx
	QUAD $0x0000000085348d4a               // leaq	(,%r8,4), %rsi
	WORD $0xff31                           // xorl	%edi, %edi
	WORD $0x894d; BYTE $0xcf               // movq	%r9, %r15
	JMP  LBB12_73

LBB12_79:
	WORD $0xff48; BYTE $0xc7 // incq	%rdi
	WORD $0x0148; BYTE $0xf2 // addq	%rsi, %rdx
	WORD $0x394c; BYTE $0xff // cmpq	%r15, %rdi
	JE   LBB12_110

LBB12_73:
	WORD $0x8949; BYTE $0xfb       // movq	%rdi, %r11
	LONG $0x5caf0f4c; WORD $0x0824 // imulq	8(%rsp), %r11                   # 8-byte Folded Reload
	LONG $0x01f88349               // cmpq	$1, %r8
	JNE  LBB12_75
	WORD $0x3145; BYTE $0xd2       // xorl	%r10d, %r10d
	LONG $0x24748b4c; BYTE $0x20   // movq	32(%rsp), %r14                  # 8-byte Reload
	JMP  LBB12_77

LBB12_75:
	LONG $0x245c8b48; BYTE $0x10 // movq	16(%rsp), %rbx                  # 8-byte Reload
	WORD $0x3145; BYTE $0xd2     // xorl	%r10d, %r10d
	LONG $0x244c8b4c; BYTE $0x08 // movq	8(%rsp), %r9                    # 8-byte Reload
	LONG $0x24748b4c; BYTE $0x20 // movq	32(%rsp), %r14                  # 8-byte Reload

LBB12_76:
	LONG $0x107c81c4; WORD $0x9e04             // vmovups	(%r14,%r11,4), %ymm0
	LONG $0x0359fcc5                           // vmulps	(%rbx), %ymm0, %ymm0
	LONG $0x197de3c4; WORD $0x01c1             // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5                           // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01               // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5                           // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5                           // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5                           // vaddss	%xmm1, %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0x9244; BYTE $0xfc // vmovss	%xmm0, -4(%rdx,%r10,4)
	LONG $0x107c81c4; WORD $0x9e04             // vmovups	(%r14,%r11,4), %ymm0
	LONG $0x597ca1c4; WORD $0x8b04             // vmulps	(%rbx,%r9,4), %ymm0, %ymm0
	LONG $0x197de3c4; WORD $0x01c1             // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5                           // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01               // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5                           // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5                           // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5                           // vaddss	%xmm1, %xmm0, %xmm0
	LONG $0x117aa1c4; WORD $0x9204             // vmovss	%xmm0, (%rdx,%r10,4)
	LONG $0x02c28349                           // addq	$2, %r10
	WORD $0x0148; BYTE $0xcb                   // addq	%rcx, %rbx
	WORD $0x394c; BYTE $0xd0                   // cmpq	%r10, %rax
	JNE  LBB12_76

LBB12_77:
	LONG $0x01c0f641               // testb	$1, %r8b
	JE   LBB12_79
	WORD $0x8949; BYTE $0xf9       // movq	%rdi, %r9
	LONG $0xc8af0f4d               // imulq	%r8, %r9
	WORD $0x894c; BYTE $0xd3       // movq	%r10, %rbx
	LONG $0x5caf0f48; WORD $0x0824 // imulq	8(%rsp), %rbx                   # 8-byte Folded Reload
	LONG $0x107c81c4; WORD $0x9e04 // vmovups	(%r14,%r11,4), %ymm0
	LONG $0x245c8b4c; BYTE $0x10   // movq	16(%rsp), %r11                  # 8-byte Reload
	LONG $0x597cc1c4; WORD $0x9b04 // vmulps	(%r11,%rbx,4), %ymm0, %ymm0
	QUAD $0x000000008d0c8d4e       // leaq	(,%r9,4), %r9
	WORD $0x014d; BYTE $0xe9       // addq	%r13, %r9
	LONG $0x197de3c4; WORD $0x01c1 // vextractf128	$1, %ymm0, %xmm1
	LONG $0xc058f0c5               // vaddps	%xmm0, %xmm1, %xmm0
	LONG $0xc8c6f9c5; BYTE $0x01   // vshufpd	$1, %xmm0, %xmm0, %xmm1         # xmm1 = xmm0[1,0]
	LONG $0xc158f8c5               // vaddps	%xmm1, %xmm0, %xmm0
	LONG $0xc816fac5               // vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	LONG $0xc158fac5               // vaddss	%xmm1, %xmm0, %xmm0
	LONG $0x117a81c4; WORD $0x9104 // vmovss	%xmm0, (%r9,%r10,4)
	LONG $0x247c8b4c; BYTE $0x28   // movq	40(%rsp), %r15                  # 8-byte Reload
	JMP  LBB12_79

LBB12_49:
	LONG $0x24448348; WORD $0x1810 // addq	$24, 16(%rsp)                   # 8-byte Folded Spill
	QUAD $0x00000000b50c8d48       // leaq	(,%rsi,4), %rcx
	QUAD $0x0000000085148d4a       // leaq	(,%r8,4), %rdx
	WORD $0xf631                   // xorl	%esi, %esi
	LONG $0xc057f8c5               // vxorps	%xmm0, %xmm0, %xmm0
	LONG $0x245c8b48; BYTE $0x28   // movq	40(%rsp), %rbx                  # 8-byte Reload
	LONG $0x244c8b4c; BYTE $0x20   // movq	32(%rsp), %r9                   # 8-byte Reload
	JMP  LBB12_50

LBB12_59:
	WORD $0xff48; BYTE $0xc6 // incq	%rsi
	WORD $0x0149; BYTE $0xd5 // addq	%rdx, %r13
	WORD $0x3948; BYTE $0xde // cmpq	%rbx, %rsi
	JE   LBB12_110

LBB12_50:
	WORD $0x8948; BYTE $0xf7       // movq	%rsi, %rdi
	LONG $0x7caf0f48; WORD $0x0824 // imulq	8(%rsp), %rdi                   # 8-byte Folded Reload
	LONG $0x24548b4c; BYTE $0x10   // movq	16(%rsp), %r10                  # 8-byte Reload
	WORD $0x3145; BYTE $0xdb       // xorl	%r11d, %r11d
	JMP  LBB12_51

LBB12_58:
	LONG $0x117a81c4; WORD $0x9d4c; BYTE $0x00 // vmovss	%xmm1, (%r13,%r11,4)
	WORD $0xff49; BYTE $0xc3                   // incq	%r11
	WORD $0x0149; BYTE $0xca                   // addq	%rcx, %r10
	WORD $0x394d; BYTE $0xd8                   // cmpq	%r11, %r8
	JE   LBB12_59

LBB12_51:
	LONG $0x107ac1c4; WORD $0xb90c             // vmovss	(%r9,%rdi,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	LONG $0x5972c1c4; WORD $0xe84a             // vmulss	-24(%r10), %xmm1, %xmm1
	LONG $0xc858f2c5                           // vaddss	%xmm0, %xmm1, %xmm1
	WORD $0xf883; BYTE $0x01                   // cmpl	$1, %eax
	JE   LBB12_58
	LONG $0x107ac1c4; WORD $0xb954; BYTE $0x04 // vmovss	4(%r9,%rdi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	LONG $0x596ac1c4; WORD $0xec52             // vmulss	-20(%r10), %xmm2, %xmm2
	LONG $0xc958eac5                           // vaddss	%xmm1, %xmm2, %xmm1
	WORD $0xf883; BYTE $0x02                   // cmpl	$2, %eax
	JE   LBB12_58
	LONG $0x107ac1c4; WORD $0xb954; BYTE $0x08 // vmovss	8(%r9,%rdi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	LONG $0x596ac1c4; WORD $0xf052             // vmulss	-16(%r10), %xmm2, %xmm2
	LONG $0xc958eac5                           // vaddss	%xmm1, %xmm2, %xmm1
	WORD $0xf883; BYTE $0x03                   // cmpl	$3, %eax
	JE   LBB12_58
	LONG $0x107ac1c4; WORD $0xb954; BYTE $0x0c // vmovss	12(%r9,%rdi,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	LONG $0x596ac1c4; WORD $0xf452             // vmulss	-12(%r10), %xmm2, %xmm2
	LONG $0xc958eac5                           // vaddss	%xmm1, %xmm2, %xmm1
	WORD $0xf883; BYTE $0x04                   // cmpl	$4, %eax
	JE   LBB12_58
	LONG $0x107ac1c4; WORD $0xb954; BYTE $0x10 // vmovss	16(%r9,%rdi,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	LONG $0x596ac1c4; WORD $0xf852             // vmulss	-8(%r10), %xmm2, %xmm2
	LONG $0xc958eac5                           // vaddss	%xmm1, %xmm2, %xmm1
	WORD $0xf883; BYTE $0x05                   // cmpl	$5, %eax
	JE   LBB12_58
	LONG $0x107ac1c4; WORD $0xb954; BYTE $0x14 // vmovss	20(%r9,%rdi,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	LONG $0x596ac1c4; WORD $0xfc52             // vmulss	-4(%r10), %xmm2, %xmm2
	LONG $0xc958eac5                           // vaddss	%xmm1, %xmm2, %xmm1
	WORD $0xf883; BYTE $0x06                   // cmpl	$6, %eax
	JE   LBB12_58
	LONG $0x107ac1c4; WORD $0xb954; BYTE $0x18 // vmovss	24(%r9,%rdi,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	LONG $0x596ac1c4; BYTE $0x12               // vmulss	(%r10), %xmm2, %xmm2
	LONG $0xc958eac5                           // vaddss	%xmm1, %xmm2, %xmm1
	JMP  LBB12_58

LBB12_111:
	LONG $0xc1af0f4d             // imulq	%r9, %r8
	LONG $0x02e0c149             // shlq	$2, %r8
	WORD $0x894c; BYTE $0xef     // movq	%r13, %rdi
	WORD $0xf631                 // xorl	%esi, %esi
	WORD $0x894c; BYTE $0xc2     // movq	%r8, %rdx
	LONG $0x000000e8; BYTE $0x00 // callq	memset@PLT

LBB12_110:
	LONG $0xd8658d48         // leaq	-40(%rbp), %rsp
	BYTE $0x5b               // popq	%rbx
	WORD $0x5c41             // popq	%r12
	WORD $0x5d41             // popq	%r13
	WORD $0x5e41             // popq	%r14
	WORD $0x5f41             // popq	%r15
	BYTE $0x5d               // popq	%rbp
	WORD $0xf8c5; BYTE $0x77 // vzeroupper
	POPQ DI
	POPQ DI
	POPQ DI
	RET
